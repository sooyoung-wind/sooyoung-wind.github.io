{"cells":[{"cell_type":"markdown","metadata":{"id":"aHru7RTkA59g"},"source":["# **<font color=white> 18.LightGBM Code 실습**\n","\n","[목적]\n","  - XGBoost Model에서 Feature와 Data를 Handling 하여 처리해주는 LightGBM Model 실습 및 해석\n","  - LightGBM의 경우 Missing Value를 Model 자체 내에서 처리해주기 때문에 삭제하지 않아도 됨\n","  - Big Data를 빠르게 학습함\n","  - 논문에서는 데이터 10,000개 이상일 때 사용하라고 했지만, 일단 돌려보자\n","\n","[Process]\n","  1. Define X's & Y\n","  2. Split Train & Valid dataset\n","  3. Modeling\n","  4. Model 해석"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":1264,"status":"ok","timestamp":1677303601717,"user":{"displayName":"안건이","userId":"00323974519415085515"},"user_tz":-540},"id":"PGwc0gKsBZKh"},"outputs":[],"source":["import os\n","import gc\n","import re\n","import pickle\n","import joblib\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import seaborn as sns\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, f1_score\n","from lightgbm import LGBMClassifier, LGBMRegressor\n","from collections import Counter"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1677303608611,"user":{"displayName":"안건이","userId":"00323974519415085515"},"user_tz":-540},"id":"ypb6iphJEdqc"},"outputs":[],"source":["# Data Loading (수술 時 사망 데이터)\n","data=pd.read_csv(\"https://raw.githubusercontent.com/GonieAhn/Data-Science-online-course-from-gonie/main/Data%20Store/example_data.csv\")"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":393},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1677303613132,"user":{"displayName":"안건이","userId":"00323974519415085515"},"user_tz":-540},"id":"FDX3EbQZEfh-","outputId":"de0df52c-d3f0-4a35-84ca-3ee651ea33b0"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>censor</th>\n","      <th>event</th>\n","      <th>age</th>\n","      <th>wtkg</th>\n","      <th>hemo</th>\n","      <th>homo</th>\n","      <th>drugs</th>\n","      <th>karnof</th>\n","      <th>oprior</th>\n","      <th>z30</th>\n","      <th>...</th>\n","      <th>gender</th>\n","      <th>str2</th>\n","      <th>strat</th>\n","      <th>symptom</th>\n","      <th>cd40</th>\n","      <th>cd420</th>\n","      <th>cd496</th>\n","      <th>r</th>\n","      <th>cd80</th>\n","      <th>cd820</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>532.000000</td>\n","      <td>532.000000</td>\n","      <td>532.000000</td>\n","      <td>532.000000</td>\n","      <td>532.000000</td>\n","      <td>532.000000</td>\n","      <td>532.000000</td>\n","      <td>532.000000</td>\n","      <td>532.000000</td>\n","      <td>532.000000</td>\n","      <td>...</td>\n","      <td>532.000000</td>\n","      <td>532.000000</td>\n","      <td>532.000000</td>\n","      <td>532.000000</td>\n","      <td>532.000000</td>\n","      <td>532.000000</td>\n","      <td>532.000000</td>\n","      <td>532.000000</td>\n","      <td>532.000000</td>\n","      <td>532.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>0.340226</td>\n","      <td>801.236842</td>\n","      <td>35.225564</td>\n","      <td>76.061855</td>\n","      <td>0.078947</td>\n","      <td>0.640977</td>\n","      <td>0.118421</td>\n","      <td>95.432331</td>\n","      <td>0.030075</td>\n","      <td>0.546992</td>\n","      <td>...</td>\n","      <td>0.812030</td>\n","      <td>0.580827</td>\n","      <td>1.981203</td>\n","      <td>0.167293</td>\n","      <td>353.204887</td>\n","      <td>336.139098</td>\n","      <td>173.146617</td>\n","      <td>0.603383</td>\n","      <td>987.250000</td>\n","      <td>928.214286</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>0.474231</td>\n","      <td>326.887929</td>\n","      <td>8.852094</td>\n","      <td>13.224698</td>\n","      <td>0.269910</td>\n","      <td>0.480165</td>\n","      <td>0.323410</td>\n","      <td>5.981856</td>\n","      <td>0.170955</td>\n","      <td>0.498255</td>\n","      <td>...</td>\n","      <td>0.391056</td>\n","      <td>0.493888</td>\n","      <td>0.905946</td>\n","      <td>0.373589</td>\n","      <td>114.105253</td>\n","      <td>130.961573</td>\n","      <td>191.455406</td>\n","      <td>0.489656</td>\n","      <td>475.223907</td>\n","      <td>438.569798</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.000000</td>\n","      <td>33.000000</td>\n","      <td>13.000000</td>\n","      <td>47.401000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>70.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>103.000000</td>\n","      <td>49.000000</td>\n","      <td>-1.000000</td>\n","      <td>0.000000</td>\n","      <td>221.000000</td>\n","      <td>150.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>0.000000</td>\n","      <td>535.750000</td>\n","      <td>29.000000</td>\n","      <td>67.500000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>90.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>271.000000</td>\n","      <td>243.750000</td>\n","      <td>-1.000000</td>\n","      <td>0.000000</td>\n","      <td>653.250000</td>\n","      <td>626.500000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>0.000000</td>\n","      <td>933.500000</td>\n","      <td>34.000000</td>\n","      <td>74.600000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>100.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>...</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>2.000000</td>\n","      <td>0.000000</td>\n","      <td>346.000000</td>\n","      <td>330.500000</td>\n","      <td>113.000000</td>\n","      <td>1.000000</td>\n","      <td>881.000000</td>\n","      <td>818.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>1.000000</td>\n","      <td>1081.000000</td>\n","      <td>40.000000</td>\n","      <td>83.502000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>100.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>...</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>3.000000</td>\n","      <td>0.000000</td>\n","      <td>422.000000</td>\n","      <td>418.000000</td>\n","      <td>324.000000</td>\n","      <td>1.000000</td>\n","      <td>1190.000000</td>\n","      <td>1164.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>1.000000</td>\n","      <td>1231.000000</td>\n","      <td>70.000000</td>\n","      <td>149.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>100.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>...</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>3.000000</td>\n","      <td>1.000000</td>\n","      <td>771.000000</td>\n","      <td>909.000000</td>\n","      <td>857.000000</td>\n","      <td>1.000000</td>\n","      <td>4255.000000</td>\n","      <td>3130.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>8 rows × 23 columns</p>\n","</div>"],"text/plain":["           censor        event         age        wtkg        hemo  \\\n","count  532.000000   532.000000  532.000000  532.000000  532.000000   \n","mean     0.340226   801.236842   35.225564   76.061855    0.078947   \n","std      0.474231   326.887929    8.852094   13.224698    0.269910   \n","min      0.000000    33.000000   13.000000   47.401000    0.000000   \n","25%      0.000000   535.750000   29.000000   67.500000    0.000000   \n","50%      0.000000   933.500000   34.000000   74.600000    0.000000   \n","75%      1.000000  1081.000000   40.000000   83.502000    0.000000   \n","max      1.000000  1231.000000   70.000000  149.000000    1.000000   \n","\n","             homo       drugs      karnof      oprior         z30  ...  \\\n","count  532.000000  532.000000  532.000000  532.000000  532.000000  ...   \n","mean     0.640977    0.118421   95.432331    0.030075    0.546992  ...   \n","std      0.480165    0.323410    5.981856    0.170955    0.498255  ...   \n","min      0.000000    0.000000   70.000000    0.000000    0.000000  ...   \n","25%      0.000000    0.000000   90.000000    0.000000    0.000000  ...   \n","50%      1.000000    0.000000  100.000000    0.000000    1.000000  ...   \n","75%      1.000000    0.000000  100.000000    0.000000    1.000000  ...   \n","max      1.000000    1.000000  100.000000    1.000000    1.000000  ...   \n","\n","           gender        str2       strat     symptom        cd40       cd420  \\\n","count  532.000000  532.000000  532.000000  532.000000  532.000000  532.000000   \n","mean     0.812030    0.580827    1.981203    0.167293  353.204887  336.139098   \n","std      0.391056    0.493888    0.905946    0.373589  114.105253  130.961573   \n","min      0.000000    0.000000    1.000000    0.000000  103.000000   49.000000   \n","25%      1.000000    0.000000    1.000000    0.000000  271.000000  243.750000   \n","50%      1.000000    1.000000    2.000000    0.000000  346.000000  330.500000   \n","75%      1.000000    1.000000    3.000000    0.000000  422.000000  418.000000   \n","max      1.000000    1.000000    3.000000    1.000000  771.000000  909.000000   \n","\n","            cd496           r         cd80        cd820  \n","count  532.000000  532.000000   532.000000   532.000000  \n","mean   173.146617    0.603383   987.250000   928.214286  \n","std    191.455406    0.489656   475.223907   438.569798  \n","min     -1.000000    0.000000   221.000000   150.000000  \n","25%     -1.000000    0.000000   653.250000   626.500000  \n","50%    113.000000    1.000000   881.000000   818.000000  \n","75%    324.000000    1.000000  1190.000000  1164.000000  \n","max    857.000000    1.000000  4255.000000  3130.000000  \n","\n","[8 rows x 23 columns]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["data.describe()"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":294,"status":"ok","timestamp":1677303640221,"user":{"displayName":"안건이","userId":"00323974519415085515"},"user_tz":-540},"id":"-FznjU_JEk1g","outputId":"f7222cdd-55c7-45b1-8666-b2020977b4d4"},"outputs":[{"name":"stdout","output_type":"stream","text":[">>>> Data Shape : (532, 22)\n"]}],"source":["# Data Quality Checking\n","col = []\n","missing = []\n","level = [] \n","for name in data.columns:\n","    \n","    # Missing\n","    missper = data[name].isnull().sum() / data.shape[0]\n","    missing.append(round(missper, 4))\n","\n","    # Leveling\n","    lel = data[name].dropna()\n","    level.append(len(list(set(lel))))\n","\n","    # Columns\n","    col.append(name)\n","\n","summary = pd.concat([pd.DataFrame(col, columns=['name']), \n","                     pd.DataFrame(missing, columns=['Missing Percentage']), \n","                     pd.DataFrame(level, columns=['Level'])], axis=1)\n","\n","drop_col = summary['name'][(summary['Level'] <= 1) | (summary['Missing Percentage'] >= 0.8)]\n","data.drop(columns=drop_col, inplace=True)\n","print(\">>>> Data Shape : {}\".format(data.shape))"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":289,"status":"ok","timestamp":1677303643436,"user":{"displayName":"안건이","userId":"00323974519415085515"},"user_tz":-540},"id":"-yg00pT3En5P","outputId":"9bee182a-84a3-4eec-8754-baf832b31b89"},"outputs":[{"data":{"text/plain":["10    zprior\n","Name: name, dtype: object"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["drop_col"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":312,"status":"ok","timestamp":1677303645552,"user":{"displayName":"안건이","userId":"00323974519415085515"},"user_tz":-540},"id":"gclolXTUEzIq"},"outputs":[],"source":["# X's & Y Split\n","Y = data['censor']\n","X = data.drop(columns=['censor'])"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1677303648074,"user":{"displayName":"안건이","userId":"00323974519415085515"},"user_tz":-540},"id":"ZAiey5vTEwaS","outputId":"8bb83e92-5d2b-4bc6-c448-f043a0a250ed"},"outputs":[{"name":"stdout","output_type":"stream","text":[">>>> # of Train data : 372\n",">>>> # of valid data : 160\n",">>>> # of Train data Y : Counter({0: 241, 1: 131})\n",">>>> # of valid data Y : Counter({0: 110, 1: 50})\n"]}],"source":["idx = list(range(X.shape[0]))\n","train_idx, valid_idx = train_test_split(idx, test_size=0.3, random_state=2021)\n","print(\">>>> # of Train data : {}\".format(len(train_idx)))\n","print(\">>>> # of valid data : {}\".format(len(valid_idx)))\n","print(\">>>> # of Train data Y : {}\".format(Counter(Y.iloc[train_idx])))\n","print(\">>>> # of valid data Y : {}\".format(Counter(Y.iloc[valid_idx])))"]},{"cell_type":"markdown","metadata":{"id":"rgnWH3OIFECw"},"source":["[LightGBM Parameters]\n","  - Package : https://lightgbm.readthedocs.io/en/latest/Python-Intro.html\n","  - learning_rate : GBM에서 shrinking 하는 것과 같은 것\n","  - reg_lambda : L2 regularization term on weights (analogous to Ridge regression)\n","  - reg_alpha : L1 regularization term on weight (analogous to Lasso regression)\n","  - objective \n","        objective 🔗︎, default = regression, type = enum, options: regression, regression_l1, huber, fair, poisson, quantile, mape, gamma, tweedie, binary, multiclass, multiclassova, cross_entropy, cross_entropy_lambda, lambdarank, rank_xendcg, aliases: objective_type, app, application, loss\n","\n","  - eval_metric [ default according to objective ]\n","    - The metric to be used for validation data.\n","    - The default values are rmse for regression and error for classification.\n","    - Typical values are:\n","        -    rmse – root mean square error\n","        -    mae – mean absolute error\n","        -    logloss – negative log-likelihood\n","        -    error – Binary classification error rate (0.5 threshold)\n","        -    merror – Multiclass classification error rate\n","        -    mlogloss – Multiclass logloss\n","        -    auc: Area under the curve"]},{"cell_type":"markdown","metadata":{"id":"69COqYEgF32e"},"source":["[LightGBM]\n","\n","  - Hyperparameter tuning\n","  - n_estimators, learning_rate, max_depth, reg_alpha\n","  - LightGBM은 Hyperparam이 굉장히 많은 알고리즘 중에 하나임\n","  - 위에 4가지만 잘 조정해도 좋은 결과를 얻을 수 있음"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":772,"status":"ok","timestamp":1677303965250,"user":{"displayName":"안건이","userId":"00323974519415085515"},"user_tz":-540},"id":"GOytS58MFDVe","outputId":"fe470348-a699-4e12-e401-bfdafe725476"},"outputs":[{"name":"stdout","output_type":"stream","text":[">>> 0 <<<\n","n_estimators : 5, learning_rate : 0.1, max_depth : 3, reg_alpha : 0.1\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004083 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Train Confusion Matrix\n","[[237   4]\n"," [ 55  76]]\n","Train Acc : 0.8413978494623656\n","Train F1-Score : 0.7203791469194314\n","Test Confusion Matrix\n","[[103   7]\n"," [ 10  40]]\n","TesT Acc : 0.89375\n","Test F1-Score : 0.8247422680412372\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 1 <<<\n","n_estimators : 5, learning_rate : 0.1, max_depth : 3, reg_alpha : 0.3\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000140 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Train Confusion Matrix\n","[[237   4]\n"," [ 54  77]]\n","Train Acc : 0.8440860215053764\n","Train F1-Score : 0.7264150943396226\n","Test Confusion Matrix\n","[[102   8]\n"," [ 10  40]]\n","TesT Acc : 0.8875\n","Test F1-Score : 0.816326530612245\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 2 <<<\n","n_estimators : 5, learning_rate : 0.1, max_depth : 3, reg_alpha : 0.5\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000179 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Train Confusion Matrix\n","[[237   4]\n"," [ 54  77]]\n","Train Acc : 0.8440860215053764\n","Train F1-Score : 0.7264150943396226\n","Test Confusion Matrix\n","[[102   8]\n"," [ 10  40]]\n","TesT Acc : 0.8875\n","Test F1-Score : 0.816326530612245\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 3 <<<\n","n_estimators : 5, learning_rate : 0.1, max_depth : 5, reg_alpha : 0.1\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000806 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Train Confusion Matrix\n","[[237   4]\n"," [ 44  87]]\n","Train Acc : 0.8709677419354839\n","Train F1-Score : 0.7837837837837838\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Test Confusion Matrix\n","[[101   9]\n"," [  9  41]]\n","TesT Acc : 0.8875\n","Test F1-Score : 0.82\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 4 <<<\n","n_estimators : 5, learning_rate : 0.1, max_depth : 5, reg_alpha : 0.3\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000207 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Train Confusion Matrix\n","[[236   5]\n"," [ 50  81]]\n","Train Acc : 0.8521505376344086\n","Train F1-Score : 0.7465437788018434\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Test Confusion Matrix\n","[[101   9]\n"," [ 10  40]]\n","TesT Acc : 0.88125\n","Test F1-Score : 0.8080808080808082\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 5 <<<\n","n_estimators : 5, learning_rate : 0.1, max_depth : 5, reg_alpha : 0.5\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000155 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Train Confusion Matrix\n","[[236   5]\n"," [ 44  87]]\n","Train Acc : 0.8682795698924731\n","Train F1-Score : 0.7802690582959643\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Test Confusion Matrix\n","[[101   9]\n"," [  9  41]]\n","TesT Acc : 0.8875\n","Test F1-Score : 0.82\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 6 <<<\n","n_estimators : 5, learning_rate : 0.3, max_depth : 3, reg_alpha : 0.1\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000206 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Train Confusion Matrix\n","[[227  14]\n"," [ 17 114]]\n","Train Acc : 0.9166666666666666\n","Train F1-Score : 0.8803088803088803\n","Test Confusion Matrix\n","[[95 15]\n"," [ 1 49]]\n","TesT Acc : 0.9\n","Test F1-Score : 0.8596491228070174\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 7 <<<\n","n_estimators : 5, learning_rate : 0.3, max_depth : 3, reg_alpha : 0.3\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000183 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Train Confusion Matrix\n","[[228  13]\n"," [ 19 112]]\n","Train Acc : 0.9139784946236559\n","Train F1-Score : 0.875\n","Test Confusion Matrix\n","[[96 14]\n"," [ 1 49]]\n","TesT Acc : 0.90625\n","Test F1-Score : 0.8672566371681417\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 8 <<<\n","n_estimators : 5, learning_rate : 0.3, max_depth : 3, reg_alpha : 0.5\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000145 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Train Confusion Matrix\n","[[228  13]\n"," [ 17 114]]\n","Train Acc : 0.9193548387096774\n","Train F1-Score : 0.883720930232558\n","Test Confusion Matrix\n","[[96 14]\n"," [ 1 49]]\n","TesT Acc : 0.90625\n","Test F1-Score : 0.8672566371681417\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 9 <<<\n","n_estimators : 5, learning_rate : 0.3, max_depth : 5, reg_alpha : 0.1\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000098 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Train Confusion Matrix\n","[[232   9]\n"," [ 20 111]]\n","Train Acc : 0.9220430107526881\n","Train F1-Score : 0.8844621513944223\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Test Confusion Matrix\n","[[95 15]\n"," [ 4 46]]\n","TesT Acc : 0.88125\n","Test F1-Score : 0.8288288288288288\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 10 <<<\n","n_estimators : 5, learning_rate : 0.3, max_depth : 5, reg_alpha : 0.3\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000187 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Train Confusion Matrix\n","[[232   9]\n"," [ 17 114]]\n","Train Acc : 0.9301075268817204\n","Train F1-Score : 0.8976377952755905\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Test Confusion Matrix\n","[[94 16]\n"," [ 5 45]]\n","TesT Acc : 0.86875\n","Test F1-Score : 0.8108108108108109\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 11 <<<\n","n_estimators : 5, learning_rate : 0.3, max_depth : 5, reg_alpha : 0.5\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000093 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Train Confusion Matrix\n","[[231  10]\n"," [ 21 110]]\n","Train Acc : 0.9166666666666666\n","Train F1-Score : 0.8764940239043825\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Test Confusion Matrix\n","[[97 13]\n"," [ 3 47]]\n","TesT Acc : 0.9\n","Test F1-Score : 0.8545454545454546\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 12 <<<\n","n_estimators : 10, learning_rate : 0.1, max_depth : 3, reg_alpha : 0.1\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000095 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Train Confusion Matrix\n","[[233   8]\n"," [ 36  95]]\n","Train Acc : 0.8817204301075269\n","Train F1-Score : 0.811965811965812\n","Test Confusion Matrix\n","[[98 12]\n"," [ 4 46]]\n","TesT Acc : 0.9\n","Test F1-Score : 0.851851851851852\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 13 <<<\n","n_estimators : 10, learning_rate : 0.1, max_depth : 3, reg_alpha : 0.3\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000090 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Train Confusion Matrix\n","[[233   8]\n"," [ 36  95]]\n","Train Acc : 0.8817204301075269\n","Train F1-Score : 0.811965811965812\n","Test Confusion Matrix\n","[[98 12]\n"," [ 4 46]]\n","TesT Acc : 0.9\n","Test F1-Score : 0.851851851851852\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 14 <<<\n","n_estimators : 10, learning_rate : 0.1, max_depth : 3, reg_alpha : 0.5\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000189 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Train Confusion Matrix\n","[[233   8]\n"," [ 33  98]]\n","Train Acc : 0.8897849462365591\n","Train F1-Score : 0.8270042194092826\n","Test Confusion Matrix\n","[[98 12]\n"," [ 5 45]]\n","TesT Acc : 0.89375\n","Test F1-Score : 0.8411214953271027\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 15 <<<\n","n_estimators : 10, learning_rate : 0.1, max_depth : 5, reg_alpha : 0.1\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000155 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Train Confusion Matrix\n","[[232   9]\n"," [ 28 103]]\n","Train Acc : 0.9005376344086021\n","Train F1-Score : 0.8477366255144032\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Test Confusion Matrix\n","[[97 13]\n"," [ 4 46]]\n","TesT Acc : 0.89375\n","Test F1-Score : 0.8440366972477064\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 16 <<<\n","n_estimators : 10, learning_rate : 0.1, max_depth : 5, reg_alpha : 0.3\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000177 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Train Confusion Matrix\n","[[233   8]\n"," [ 25 106]]\n","Train Acc : 0.9112903225806451\n","Train F1-Score : 0.8653061224489796\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Test Confusion Matrix\n","[[96 14]\n"," [ 4 46]]\n","TesT Acc : 0.8875\n","Test F1-Score : 0.8363636363636363\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 17 <<<\n","n_estimators : 10, learning_rate : 0.1, max_depth : 5, reg_alpha : 0.5\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000109 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Train Confusion Matrix\n","[[232   9]\n"," [ 28 103]]\n","Train Acc : 0.9005376344086021\n","Train F1-Score : 0.8477366255144032\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Test Confusion Matrix\n","[[97 13]\n"," [ 4 46]]\n","TesT Acc : 0.89375\n","Test F1-Score : 0.8440366972477064\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 18 <<<\n","n_estimators : 10, learning_rate : 0.3, max_depth : 3, reg_alpha : 0.1\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000227 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Train Confusion Matrix\n","[[227  14]\n"," [ 12 119]]\n","Train Acc : 0.9301075268817204\n","Train F1-Score : 0.9015151515151515\n","Test Confusion Matrix\n","[[92 18]\n"," [ 1 49]]\n","TesT Acc : 0.88125\n","Test F1-Score : 0.8376068376068375\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 19 <<<\n","n_estimators : 10, learning_rate : 0.3, max_depth : 3, reg_alpha : 0.3\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000109 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Train Confusion Matrix\n","[[229  12]\n"," [ 10 121]]\n","Train Acc : 0.9408602150537635\n","Train F1-Score : 0.9166666666666667\n","Test Confusion Matrix\n","[[92 18]\n"," [ 1 49]]\n","TesT Acc : 0.88125\n","Test F1-Score : 0.8376068376068375\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 20 <<<\n","n_estimators : 10, learning_rate : 0.3, max_depth : 3, reg_alpha : 0.5\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000079 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Train Confusion Matrix\n","[[229  12]\n"," [ 13 118]]\n","Train Acc : 0.9327956989247311\n","Train F1-Score : 0.9042145593869731\n","Test Confusion Matrix\n","[[92 18]\n"," [ 1 49]]\n","TesT Acc : 0.88125\n","Test F1-Score : 0.8376068376068375\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 21 <<<\n","n_estimators : 10, learning_rate : 0.3, max_depth : 5, reg_alpha : 0.1\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000092 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Train Confusion Matrix\n","[[233   8]\n"," [ 12 119]]\n","Train Acc : 0.946236559139785\n","Train F1-Score : 0.9224806201550387\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Test Confusion Matrix\n","[[98 12]\n"," [ 3 47]]\n","TesT Acc : 0.90625\n","Test F1-Score : 0.8623853211009174\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 22 <<<\n","n_estimators : 10, learning_rate : 0.3, max_depth : 5, reg_alpha : 0.3\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000179 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Train Confusion Matrix\n","[[234   7]\n"," [ 10 121]]\n","Train Acc : 0.9543010752688172\n","Train F1-Score : 0.9343629343629344\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Test Confusion Matrix\n","[[96 14]\n"," [ 4 46]]\n","TesT Acc : 0.8875\n","Test F1-Score : 0.8363636363636363\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 23 <<<\n","n_estimators : 10, learning_rate : 0.3, max_depth : 5, reg_alpha : 0.5\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000086 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Train Confusion Matrix\n","[[234   7]\n"," [ 11 120]]\n","Train Acc : 0.9516129032258065\n","Train F1-Score : 0.9302325581395349\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Test Confusion Matrix\n","[[96 14]\n"," [ 4 46]]\n","TesT Acc : 0.8875\n","Test F1-Score : 0.8363636363636363\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 24 <<<\n","n_estimators : 20, learning_rate : 0.1, max_depth : 3, reg_alpha : 0.1\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000144 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Train Confusion Matrix\n","[[229  12]\n"," [ 17 114]]\n","Train Acc : 0.9220430107526881\n","Train F1-Score : 0.8871595330739299\n","Test Confusion Matrix\n","[[95 15]\n"," [ 2 48]]\n","TesT Acc : 0.89375\n","Test F1-Score : 0.8495575221238937\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 25 <<<\n","n_estimators : 20, learning_rate : 0.1, max_depth : 3, reg_alpha : 0.3\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000155 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Train Confusion Matrix\n","[[229  12]\n"," [ 14 117]]\n","Train Acc : 0.9301075268817204\n","Train F1-Score : 0.9\n","Test Confusion Matrix\n","[[94 16]\n"," [ 1 49]]\n","TesT Acc : 0.89375\n","Test F1-Score : 0.8521739130434782\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 26 <<<\n","n_estimators : 20, learning_rate : 0.1, max_depth : 3, reg_alpha : 0.5\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000077 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Train Confusion Matrix\n","[[228  13]\n"," [ 13 118]]\n","Train Acc : 0.9301075268817204\n","Train F1-Score : 0.9007633587786259\n","Test Confusion Matrix\n","[[94 16]\n"," [ 1 49]]\n","TesT Acc : 0.89375\n","Test F1-Score : 0.8521739130434782\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 27 <<<\n","n_estimators : 20, learning_rate : 0.1, max_depth : 5, reg_alpha : 0.1\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000159 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Train Confusion Matrix\n","[[233   8]\n"," [ 15 116]]\n","Train Acc : 0.9381720430107527\n","Train F1-Score : 0.9098039215686274\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Test Confusion Matrix\n","[[95 15]\n"," [ 3 47]]\n","TesT Acc : 0.8875\n","Test F1-Score : 0.8392857142857143\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 28 <<<\n","n_estimators : 20, learning_rate : 0.1, max_depth : 5, reg_alpha : 0.3\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000169 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Train Confusion Matrix\n","[[232   9]\n"," [ 14 117]]\n","Train Acc : 0.9381720430107527\n","Train F1-Score : 0.9105058365758756\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Test Confusion Matrix\n","[[95 15]\n"," [ 2 48]]\n","TesT Acc : 0.89375\n","Test F1-Score : 0.8495575221238937\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 29 <<<\n","n_estimators : 20, learning_rate : 0.1, max_depth : 5, reg_alpha : 0.5\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000609 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Train Confusion Matrix\n","[[232   9]\n"," [ 17 114]]\n","Train Acc : 0.9301075268817204\n","Train F1-Score : 0.8976377952755905\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Test Confusion Matrix\n","[[96 14]\n"," [ 3 47]]\n","TesT Acc : 0.89375\n","Test F1-Score : 0.8468468468468469\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 30 <<<\n","n_estimators : 20, learning_rate : 0.3, max_depth : 3, reg_alpha : 0.1\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000194 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Train Confusion Matrix\n","[[236   5]\n"," [  6 125]]\n","Train Acc : 0.9704301075268817\n","Train F1-Score : 0.9578544061302683\n","Test Confusion Matrix\n","[[89 21]\n"," [ 1 49]]\n","TesT Acc : 0.8625\n","Test F1-Score : 0.8166666666666667\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 31 <<<\n","n_estimators : 20, learning_rate : 0.3, max_depth : 3, reg_alpha : 0.3\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000097 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Train Confusion Matrix\n","[[233   8]\n"," [  6 125]]\n","Train Acc : 0.9623655913978495\n","Train F1-Score : 0.9469696969696969\n","Test Confusion Matrix\n","[[91 19]\n"," [ 1 49]]\n","TesT Acc : 0.875\n","Test F1-Score : 0.8305084745762712\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 32 <<<\n","n_estimators : 20, learning_rate : 0.3, max_depth : 3, reg_alpha : 0.5\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000180 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Train Confusion Matrix\n","[[234   7]\n"," [  8 123]]\n","Train Acc : 0.9596774193548387\n","Train F1-Score : 0.9425287356321839\n","Test Confusion Matrix\n","[[93 17]\n"," [ 1 49]]\n","TesT Acc : 0.8875\n","Test F1-Score : 0.8448275862068965\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 33 <<<\n","n_estimators : 20, learning_rate : 0.3, max_depth : 5, reg_alpha : 0.1\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000090 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Train Confusion Matrix\n","[[239   2]\n"," [  2 129]]\n","Train Acc : 0.989247311827957\n","Train F1-Score : 0.9847328244274809\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Test Confusion Matrix\n","[[94 16]\n"," [ 2 48]]\n","TesT Acc : 0.8875\n","Test F1-Score : 0.8421052631578947\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 34 <<<\n","n_estimators : 20, learning_rate : 0.3, max_depth : 5, reg_alpha : 0.3\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000153 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Train Confusion Matrix\n","[[237   4]\n"," [  0 131]]\n","Train Acc : 0.989247311827957\n","Train F1-Score : 0.9849624060150376\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Test Confusion Matrix\n","[[93 17]\n"," [ 2 48]]\n","TesT Acc : 0.88125\n","Test F1-Score : 0.8347826086956522\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 35 <<<\n","n_estimators : 20, learning_rate : 0.3, max_depth : 5, reg_alpha : 0.5\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000096 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Train Confusion Matrix\n","[[237   4]\n"," [  1 130]]\n","Train Acc : 0.9865591397849462\n","Train F1-Score : 0.9811320754716981\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Test Confusion Matrix\n","[[92 18]\n"," [ 4 46]]\n","TesT Acc : 0.8625\n","Test F1-Score : 0.8070175438596492\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n"]}],"source":["# n_estimators\n","n_tree = [5, 10, 20]\n","# learning_rate\n","l_rate = [0.1, 0.3]\n","# max_depth\n","m_depth = [3, 5]\n","# reg_alpha\n","L1_norm = [0.1, 0.3, 0.5]\n","\n","# Modeling\n","save_n = []\n","save_l = []\n","save_m = []\n","save_L1 = []\n","f1_score_ = []\n","\n","cnt = 0\n","\n","for n in n_tree:\n","    for l in l_rate:\n","        for m in m_depth:\n","            for L1 in L1_norm:\n","                \n","                print(\">>> {} <<<\".format(cnt))\n","                cnt +=1\n","                print(\"n_estimators : {}, learning_rate : {}, max_depth : {}, reg_alpha : {}\".format(n, l, m, L1))\n","                model = LGBMClassifier(n_estimators=n, learning_rate=l, \n","                                       max_depth=m, reg_alpha=L1, \n","                                       n_jobs=-1, objective='cross_entropy')\n","                model.fit(X.iloc[train_idx], Y.iloc[train_idx])\n","                \n","                \n","                # Train Acc\n","                y_pre_train = model.predict(X.iloc[train_idx])\n","                cm_train = confusion_matrix(Y.iloc[train_idx], y_pre_train)\n","                print(\"Train Confusion Matrix\")\n","                print(cm_train)\n","                print(\"Train Acc : {}\".format((cm_train[0,0] + cm_train[1,1])/cm_train.sum()))\n","                print(\"Train F1-Score : {}\".format(f1_score(Y.iloc[train_idx], y_pre_train)))\n","\n","                # Test Acc\n","                y_pre_test = model.predict(X.iloc[valid_idx])\n","                cm_test = confusion_matrix(Y.iloc[valid_idx], y_pre_test)\n","                print(\"Test Confusion Matrix\")\n","                print(cm_test)\n","                print(\"TesT Acc : {}\".format((cm_test[0,0] + cm_test[1,1])/cm_test.sum()))\n","                print(\"Test F1-Score : {}\".format(f1_score(Y.iloc[valid_idx], y_pre_test)))\n","                print(\"-----------------------------------------------------------------------\")\n","                print(\"-----------------------------------------------------------------------\")\n","                save_n.append(n)\n","                save_l.append(l)\n","                save_m.append(m)\n","                save_L1.append(L1)\n","                f1_score_.append(f1_score(Y.iloc[valid_idx], y_pre_test))\n","\n","\n","                #joblib.dump(model, './LightGBM_model/Result_{}_{}_{}_{}_{}.pkl'.format(n, l, m, L1, round(save_acc[-1], 4)))\n","                #gc.collect()"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":579,"status":"ok","timestamp":1677304026710,"user":{"displayName":"안건이","userId":"00323974519415085515"},"user_tz":-540},"id":"kSwDemERG4k3","outputId":"f151e7db-6eaa-4c9d-edbe-fcfd67060ed9"},"outputs":[{"name":"stdout","output_type":"stream","text":[">>> 7 <<<\n","Best Test f1-score : 0.8672566371681417\n","Best n_estimators : 5\n","Best Learning Rate : 0.3\n","Best Max_depth : 3\n","Best L1-norm : 0.3\n"]}],"source":["print(\">>> {} <<<\\nBest Test f1-score : {}\\nBest n_estimators : {}\\nBest Learning Rate : {}\\nBest Max_depth : {}\\nBest L1-norm : {}\".format(np.argmax(f1_score_),\n","                                                                                                                                            f1_score_[np.argmax(f1_score_)], \n","                                                                                                                                            save_n[np.argmax(f1_score_)],\n","                                                                                                                                            save_l[np.argmax(f1_score_)],\n","                                                                                                                                            save_m[np.argmax(f1_score_)],\n","                                                                                                                                            save_L1[np.argmax(f1_score_)]))"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":289,"status":"ok","timestamp":1677304166877,"user":{"displayName":"안건이","userId":"00323974519415085515"},"user_tz":-540},"id":"uxOZPKUaEzfC","outputId":"4a6db841-c543-430e-ddd4-3ad06f0b7a35"},"outputs":[{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000194 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Train Confusion Matrix\n","[[228  13]\n"," [ 19 112]]\n","Train Acc : 0.9139784946236559\n","Train F1-Score : 0.875\n","Test Confusion Matrix\n","[[96 14]\n"," [ 1 49]]\n","TesT Acc : 0.90625\n","Test F1-Score : 0.8672566371681417\n"]}],"source":["best_model = LGBMClassifier(n_estimators=save_n[np.argmax(f1_score_)], learning_rate=save_l[np.argmax(f1_score_)], \n","                           max_depth=save_m[np.argmax(f1_score_)], reg_alpha=save_L1[np.argmax(f1_score_)], objective='cross_entropy', \n","                           random_state=119)\n","best_model.fit(X.iloc[train_idx], Y.iloc[train_idx])\n","\n","# Train Acc\n","y_pre_train = best_model.predict(X.iloc[train_idx])\n","cm_train = confusion_matrix(Y.iloc[train_idx], y_pre_train)\n","print(\"Train Confusion Matrix\")\n","print(cm_train)\n","print(\"Train Acc : {}\".format((cm_train[0,0] + cm_train[1,1])/cm_train.sum()))\n","print(\"Train F1-Score : {}\".format(f1_score(Y.iloc[train_idx], y_pre_train)))\n","\n","# Test Acc\n","y_pre_test = best_model.predict(X.iloc[valid_idx])\n","cm_test = confusion_matrix(Y.iloc[valid_idx], y_pre_test)\n","print(\"Test Confusion Matrix\")\n","print(cm_test)\n","print(\"TesT Acc : {}\".format((cm_test[0,0] + cm_test[1,1])/cm_test.sum()))\n","print(\"Test F1-Score : {}\".format(f1_score(Y.iloc[valid_idx], y_pre_test)))"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":343,"status":"ok","timestamp":1677304174236,"user":{"displayName":"안건이","userId":"00323974519415085515"},"user_tz":-540},"id":"GUl8n94_HEIL","outputId":"f11d41dc-dd3d-42ce-eff1-5cc033cb7ff7"},"outputs":[{"name":"stdout","output_type":"stream","text":["    Score  Feature\n","0      10    event\n","1       4    cd496\n","2       4    cd420\n","3       4     cd40\n","4       3  preanti\n","5       2     wtkg\n","6       2     race\n","7       1      z30\n","8       1      age\n","9       0  symptom\n","10      0    strat\n","11      0     str2\n","12      0        r\n","13      0   oprior\n","14      0   karnof\n","15      0     homo\n","16      0     hemo\n","17      0   gender\n","18      0    drugs\n","19      0    cd820\n","20      0     cd80\n"]}],"source":["feature_map = pd.DataFrame(sorted(zip(best_model.feature_importances_, X.columns), reverse=True), columns=['Score', 'Feature'])\n","print(feature_map)"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":720},"executionInfo":{"elapsed":778,"status":"ok","timestamp":1677304178102,"user":{"displayName":"안건이","userId":"00323974519415085515"},"user_tz":-540},"id":"Zup8IF1zHHCN","outputId":"7103bb58-2c67-491a-e3ba-4fd26aa350f1"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 2000x1000 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# Importance Score Top 10\n","feature_map_10 = feature_map.iloc[:10]\n","plt.figure(figsize=(20, 10))\n","sns.barplot(x=\"Score\", y=\"Feature\", data=feature_map_10.sort_values(by=\"Score\", ascending=False), errwidth=40)\n","plt.title('XGBoost Importance Features')\n","plt.tight_layout()\n","plt.show()"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOYjPhV7X11SOAtaishDu7i","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
