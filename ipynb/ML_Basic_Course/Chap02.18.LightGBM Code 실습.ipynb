{"cells":[{"cell_type":"markdown","metadata":{"id":"aHru7RTkA59g"},"source":["# **<font color=white> 18.LightGBM Code Ïã§Ïäµ**\n","\n","[Î™©Ï†Å]\n","  - XGBoost ModelÏóêÏÑú FeatureÏôÄ DataÎ•º Handling ÌïòÏó¨ Ï≤òÎ¶¨Ìï¥Ï£ºÎäî LightGBM Model Ïã§Ïäµ Î∞è Ìï¥ÏÑù\n","  - LightGBMÏùò Í≤ΩÏö∞ Missing ValueÎ•º Model ÏûêÏ≤¥ ÎÇ¥ÏóêÏÑú Ï≤òÎ¶¨Ìï¥Ï£ºÍ∏∞ ÎïåÎ¨∏Ïóê ÏÇ≠Ï†úÌïòÏßÄ ÏïäÏïÑÎèÑ Îê®\n","  - Big DataÎ•º Îπ†Î•¥Í≤å ÌïôÏäµÌï®\n","  - ÎÖºÎ¨∏ÏóêÏÑúÎäî Îç∞Ïù¥ÌÑ∞ 10,000Í∞ú Ïù¥ÏÉÅÏùº Îïå ÏÇ¨Ïö©ÌïòÎùºÍ≥† ÌñàÏßÄÎßå, ÏùºÎã® ÎèåÎ†§Î≥¥Ïûê\n","\n","[Process]\n","  1. Define X's & Y\n","  2. Split Train & Valid dataset\n","  3. Modeling\n","  4. Model Ìï¥ÏÑù"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":1264,"status":"ok","timestamp":1677303601717,"user":{"displayName":"ÏïàÍ±¥Ïù¥","userId":"00323974519415085515"},"user_tz":-540},"id":"PGwc0gKsBZKh"},"outputs":[],"source":["import os\n","import gc\n","import re\n","import pickle\n","import joblib\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import seaborn as sns\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, f1_score\n","from lightgbm import LGBMClassifier, LGBMRegressor\n","from collections import Counter"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1677303608611,"user":{"displayName":"ÏïàÍ±¥Ïù¥","userId":"00323974519415085515"},"user_tz":-540},"id":"ypb6iphJEdqc"},"outputs":[],"source":["# Data Loading (ÏàòÏà† ÊôÇ ÏÇ¨Îßù Îç∞Ïù¥ÌÑ∞)\n","data=pd.read_csv(\"https://raw.githubusercontent.com/GonieAhn/Data-Science-online-course-from-gonie/main/Data%20Store/example_data.csv\")"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":393},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1677303613132,"user":{"displayName":"ÏïàÍ±¥Ïù¥","userId":"00323974519415085515"},"user_tz":-540},"id":"FDX3EbQZEfh-","outputId":"de0df52c-d3f0-4a35-84ca-3ee651ea33b0"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>censor</th>\n","      <th>event</th>\n","      <th>age</th>\n","      <th>wtkg</th>\n","      <th>hemo</th>\n","      <th>homo</th>\n","      <th>drugs</th>\n","      <th>karnof</th>\n","      <th>oprior</th>\n","      <th>z30</th>\n","      <th>...</th>\n","      <th>gender</th>\n","      <th>str2</th>\n","      <th>strat</th>\n","      <th>symptom</th>\n","      <th>cd40</th>\n","      <th>cd420</th>\n","      <th>cd496</th>\n","      <th>r</th>\n","      <th>cd80</th>\n","      <th>cd820</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>532.000000</td>\n","      <td>532.000000</td>\n","      <td>532.000000</td>\n","      <td>532.000000</td>\n","      <td>532.000000</td>\n","      <td>532.000000</td>\n","      <td>532.000000</td>\n","      <td>532.000000</td>\n","      <td>532.000000</td>\n","      <td>532.000000</td>\n","      <td>...</td>\n","      <td>532.000000</td>\n","      <td>532.000000</td>\n","      <td>532.000000</td>\n","      <td>532.000000</td>\n","      <td>532.000000</td>\n","      <td>532.000000</td>\n","      <td>532.000000</td>\n","      <td>532.000000</td>\n","      <td>532.000000</td>\n","      <td>532.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>0.340226</td>\n","      <td>801.236842</td>\n","      <td>35.225564</td>\n","      <td>76.061855</td>\n","      <td>0.078947</td>\n","      <td>0.640977</td>\n","      <td>0.118421</td>\n","      <td>95.432331</td>\n","      <td>0.030075</td>\n","      <td>0.546992</td>\n","      <td>...</td>\n","      <td>0.812030</td>\n","      <td>0.580827</td>\n","      <td>1.981203</td>\n","      <td>0.167293</td>\n","      <td>353.204887</td>\n","      <td>336.139098</td>\n","      <td>173.146617</td>\n","      <td>0.603383</td>\n","      <td>987.250000</td>\n","      <td>928.214286</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>0.474231</td>\n","      <td>326.887929</td>\n","      <td>8.852094</td>\n","      <td>13.224698</td>\n","      <td>0.269910</td>\n","      <td>0.480165</td>\n","      <td>0.323410</td>\n","      <td>5.981856</td>\n","      <td>0.170955</td>\n","      <td>0.498255</td>\n","      <td>...</td>\n","      <td>0.391056</td>\n","      <td>0.493888</td>\n","      <td>0.905946</td>\n","      <td>0.373589</td>\n","      <td>114.105253</td>\n","      <td>130.961573</td>\n","      <td>191.455406</td>\n","      <td>0.489656</td>\n","      <td>475.223907</td>\n","      <td>438.569798</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.000000</td>\n","      <td>33.000000</td>\n","      <td>13.000000</td>\n","      <td>47.401000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>70.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>103.000000</td>\n","      <td>49.000000</td>\n","      <td>-1.000000</td>\n","      <td>0.000000</td>\n","      <td>221.000000</td>\n","      <td>150.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>0.000000</td>\n","      <td>535.750000</td>\n","      <td>29.000000</td>\n","      <td>67.500000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>90.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>271.000000</td>\n","      <td>243.750000</td>\n","      <td>-1.000000</td>\n","      <td>0.000000</td>\n","      <td>653.250000</td>\n","      <td>626.500000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>0.000000</td>\n","      <td>933.500000</td>\n","      <td>34.000000</td>\n","      <td>74.600000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>100.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>...</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>2.000000</td>\n","      <td>0.000000</td>\n","      <td>346.000000</td>\n","      <td>330.500000</td>\n","      <td>113.000000</td>\n","      <td>1.000000</td>\n","      <td>881.000000</td>\n","      <td>818.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>1.000000</td>\n","      <td>1081.000000</td>\n","      <td>40.000000</td>\n","      <td>83.502000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>100.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>...</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>3.000000</td>\n","      <td>0.000000</td>\n","      <td>422.000000</td>\n","      <td>418.000000</td>\n","      <td>324.000000</td>\n","      <td>1.000000</td>\n","      <td>1190.000000</td>\n","      <td>1164.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>1.000000</td>\n","      <td>1231.000000</td>\n","      <td>70.000000</td>\n","      <td>149.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>100.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>...</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>3.000000</td>\n","      <td>1.000000</td>\n","      <td>771.000000</td>\n","      <td>909.000000</td>\n","      <td>857.000000</td>\n","      <td>1.000000</td>\n","      <td>4255.000000</td>\n","      <td>3130.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>8 rows √ó 23 columns</p>\n","</div>"],"text/plain":["           censor        event         age        wtkg        hemo  \\\n","count  532.000000   532.000000  532.000000  532.000000  532.000000   \n","mean     0.340226   801.236842   35.225564   76.061855    0.078947   \n","std      0.474231   326.887929    8.852094   13.224698    0.269910   \n","min      0.000000    33.000000   13.000000   47.401000    0.000000   \n","25%      0.000000   535.750000   29.000000   67.500000    0.000000   \n","50%      0.000000   933.500000   34.000000   74.600000    0.000000   \n","75%      1.000000  1081.000000   40.000000   83.502000    0.000000   \n","max      1.000000  1231.000000   70.000000  149.000000    1.000000   \n","\n","             homo       drugs      karnof      oprior         z30  ...  \\\n","count  532.000000  532.000000  532.000000  532.000000  532.000000  ...   \n","mean     0.640977    0.118421   95.432331    0.030075    0.546992  ...   \n","std      0.480165    0.323410    5.981856    0.170955    0.498255  ...   \n","min      0.000000    0.000000   70.000000    0.000000    0.000000  ...   \n","25%      0.000000    0.000000   90.000000    0.000000    0.000000  ...   \n","50%      1.000000    0.000000  100.000000    0.000000    1.000000  ...   \n","75%      1.000000    0.000000  100.000000    0.000000    1.000000  ...   \n","max      1.000000    1.000000  100.000000    1.000000    1.000000  ...   \n","\n","           gender        str2       strat     symptom        cd40       cd420  \\\n","count  532.000000  532.000000  532.000000  532.000000  532.000000  532.000000   \n","mean     0.812030    0.580827    1.981203    0.167293  353.204887  336.139098   \n","std      0.391056    0.493888    0.905946    0.373589  114.105253  130.961573   \n","min      0.000000    0.000000    1.000000    0.000000  103.000000   49.000000   \n","25%      1.000000    0.000000    1.000000    0.000000  271.000000  243.750000   \n","50%      1.000000    1.000000    2.000000    0.000000  346.000000  330.500000   \n","75%      1.000000    1.000000    3.000000    0.000000  422.000000  418.000000   \n","max      1.000000    1.000000    3.000000    1.000000  771.000000  909.000000   \n","\n","            cd496           r         cd80        cd820  \n","count  532.000000  532.000000   532.000000   532.000000  \n","mean   173.146617    0.603383   987.250000   928.214286  \n","std    191.455406    0.489656   475.223907   438.569798  \n","min     -1.000000    0.000000   221.000000   150.000000  \n","25%     -1.000000    0.000000   653.250000   626.500000  \n","50%    113.000000    1.000000   881.000000   818.000000  \n","75%    324.000000    1.000000  1190.000000  1164.000000  \n","max    857.000000    1.000000  4255.000000  3130.000000  \n","\n","[8 rows x 23 columns]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["data.describe()"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":294,"status":"ok","timestamp":1677303640221,"user":{"displayName":"ÏïàÍ±¥Ïù¥","userId":"00323974519415085515"},"user_tz":-540},"id":"-FznjU_JEk1g","outputId":"f7222cdd-55c7-45b1-8666-b2020977b4d4"},"outputs":[{"name":"stdout","output_type":"stream","text":[">>>> Data Shape : (532, 22)\n"]}],"source":["# Data Quality Checking\n","col = []\n","missing = []\n","level = [] \n","for name in data.columns:\n","    \n","    # Missing\n","    missper = data[name].isnull().sum() / data.shape[0]\n","    missing.append(round(missper, 4))\n","\n","    # Leveling\n","    lel = data[name].dropna()\n","    level.append(len(list(set(lel))))\n","\n","    # Columns\n","    col.append(name)\n","\n","summary = pd.concat([pd.DataFrame(col, columns=['name']), \n","                     pd.DataFrame(missing, columns=['Missing Percentage']), \n","                     pd.DataFrame(level, columns=['Level'])], axis=1)\n","\n","drop_col = summary['name'][(summary['Level'] <= 1) | (summary['Missing Percentage'] >= 0.8)]\n","data.drop(columns=drop_col, inplace=True)\n","print(\">>>> Data Shape : {}\".format(data.shape))"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":289,"status":"ok","timestamp":1677303643436,"user":{"displayName":"ÏïàÍ±¥Ïù¥","userId":"00323974519415085515"},"user_tz":-540},"id":"-yg00pT3En5P","outputId":"9bee182a-84a3-4eec-8754-baf832b31b89"},"outputs":[{"data":{"text/plain":["10    zprior\n","Name: name, dtype: object"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["drop_col"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":312,"status":"ok","timestamp":1677303645552,"user":{"displayName":"ÏïàÍ±¥Ïù¥","userId":"00323974519415085515"},"user_tz":-540},"id":"gclolXTUEzIq"},"outputs":[],"source":["# X's & Y Split\n","Y = data['censor']\n","X = data.drop(columns=['censor'])"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1677303648074,"user":{"displayName":"ÏïàÍ±¥Ïù¥","userId":"00323974519415085515"},"user_tz":-540},"id":"ZAiey5vTEwaS","outputId":"8bb83e92-5d2b-4bc6-c448-f043a0a250ed"},"outputs":[{"name":"stdout","output_type":"stream","text":[">>>> # of Train data : 372\n",">>>> # of valid data : 160\n",">>>> # of Train data Y : Counter({0: 241, 1: 131})\n",">>>> # of valid data Y : Counter({0: 110, 1: 50})\n"]}],"source":["idx = list(range(X.shape[0]))\n","train_idx, valid_idx = train_test_split(idx, test_size=0.3, random_state=2021)\n","print(\">>>> # of Train data : {}\".format(len(train_idx)))\n","print(\">>>> # of valid data : {}\".format(len(valid_idx)))\n","print(\">>>> # of Train data Y : {}\".format(Counter(Y.iloc[train_idx])))\n","print(\">>>> # of valid data Y : {}\".format(Counter(Y.iloc[valid_idx])))"]},{"cell_type":"markdown","metadata":{"id":"rgnWH3OIFECw"},"source":["[LightGBM Parameters]\n","  - Package : https://lightgbm.readthedocs.io/en/latest/Python-Intro.html\n","  - learning_rate : GBMÏóêÏÑú shrinking ÌïòÎäî Í≤ÉÍ≥º Í∞ôÏùÄ Í≤É\n","  - reg_lambda : L2 regularization term on weights (analogous to Ridge regression)\n","  - reg_alpha : L1 regularization term on weight (analogous to Lasso regression)\n","  - objective \n","        objective üîóÔ∏é, default = regression, type = enum, options: regression, regression_l1, huber, fair, poisson, quantile, mape, gamma, tweedie, binary, multiclass, multiclassova, cross_entropy, cross_entropy_lambda, lambdarank, rank_xendcg, aliases: objective_type, app, application, loss\n","\n","  - eval_metric [ default according to objective ]\n","    - The metric to be used for validation data.\n","    - The default values are rmse for regression and error for classification.\n","    - Typical values are:\n","        -    rmse ‚Äì root mean square error\n","        -    mae ‚Äì mean absolute error\n","        -    logloss ‚Äì negative log-likelihood\n","        -    error ‚Äì Binary classification error rate (0.5 threshold)\n","        -    merror ‚Äì Multiclass classification error rate\n","        -    mlogloss ‚Äì Multiclass logloss\n","        -    auc: Area under the curve"]},{"cell_type":"markdown","metadata":{"id":"69COqYEgF32e"},"source":["[LightGBM]\n","\n","  - Hyperparameter tuning\n","  - n_estimators, learning_rate, max_depth, reg_alpha\n","  - LightGBMÏùÄ HyperparamÏù¥ ÍµâÏû•Ìûà ÎßéÏùÄ ÏïåÍ≥†Î¶¨Ï¶ò Ï§ëÏóê ÌïòÎÇòÏûÑ\n","  - ÏúÑÏóê 4Í∞ÄÏßÄÎßå Ïûò Ï°∞Ï†ïÌï¥ÎèÑ Ï¢ãÏùÄ Í≤∞Í≥ºÎ•º ÏñªÏùÑ Ïàò ÏûàÏùå"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":772,"status":"ok","timestamp":1677303965250,"user":{"displayName":"ÏïàÍ±¥Ïù¥","userId":"00323974519415085515"},"user_tz":-540},"id":"GOytS58MFDVe","outputId":"fe470348-a699-4e12-e401-bfdafe725476"},"outputs":[{"name":"stdout","output_type":"stream","text":[">>> 0 <<<\n","n_estimators : 5, learning_rate : 0.1, max_depth : 3, reg_alpha : 0.1\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004083 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Train Confusion Matrix\n","[[237   4]\n"," [ 55  76]]\n","Train Acc : 0.8413978494623656\n","Train F1-Score : 0.7203791469194314\n","Test Confusion Matrix\n","[[103   7]\n"," [ 10  40]]\n","TesT Acc : 0.89375\n","Test F1-Score : 0.8247422680412372\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 1 <<<\n","n_estimators : 5, learning_rate : 0.1, max_depth : 3, reg_alpha : 0.3\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000140 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Train Confusion Matrix\n","[[237   4]\n"," [ 54  77]]\n","Train Acc : 0.8440860215053764\n","Train F1-Score : 0.7264150943396226\n","Test Confusion Matrix\n","[[102   8]\n"," [ 10  40]]\n","TesT Acc : 0.8875\n","Test F1-Score : 0.816326530612245\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 2 <<<\n","n_estimators : 5, learning_rate : 0.1, max_depth : 3, reg_alpha : 0.5\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000179 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Train Confusion Matrix\n","[[237   4]\n"," [ 54  77]]\n","Train Acc : 0.8440860215053764\n","Train F1-Score : 0.7264150943396226\n","Test Confusion Matrix\n","[[102   8]\n"," [ 10  40]]\n","TesT Acc : 0.8875\n","Test F1-Score : 0.816326530612245\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 3 <<<\n","n_estimators : 5, learning_rate : 0.1, max_depth : 5, reg_alpha : 0.1\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000806 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Train Confusion Matrix\n","[[237   4]\n"," [ 44  87]]\n","Train Acc : 0.8709677419354839\n","Train F1-Score : 0.7837837837837838\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Test Confusion Matrix\n","[[101   9]\n"," [  9  41]]\n","TesT Acc : 0.8875\n","Test F1-Score : 0.82\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 4 <<<\n","n_estimators : 5, learning_rate : 0.1, max_depth : 5, reg_alpha : 0.3\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000207 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Train Confusion Matrix\n","[[236   5]\n"," [ 50  81]]\n","Train Acc : 0.8521505376344086\n","Train F1-Score : 0.7465437788018434\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Test Confusion Matrix\n","[[101   9]\n"," [ 10  40]]\n","TesT Acc : 0.88125\n","Test F1-Score : 0.8080808080808082\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 5 <<<\n","n_estimators : 5, learning_rate : 0.1, max_depth : 5, reg_alpha : 0.5\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000155 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Train Confusion Matrix\n","[[236   5]\n"," [ 44  87]]\n","Train Acc : 0.8682795698924731\n","Train F1-Score : 0.7802690582959643\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Test Confusion Matrix\n","[[101   9]\n"," [  9  41]]\n","TesT Acc : 0.8875\n","Test F1-Score : 0.82\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 6 <<<\n","n_estimators : 5, learning_rate : 0.3, max_depth : 3, reg_alpha : 0.1\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000206 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Train Confusion Matrix\n","[[227  14]\n"," [ 17 114]]\n","Train Acc : 0.9166666666666666\n","Train F1-Score : 0.8803088803088803\n","Test Confusion Matrix\n","[[95 15]\n"," [ 1 49]]\n","TesT Acc : 0.9\n","Test F1-Score : 0.8596491228070174\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 7 <<<\n","n_estimators : 5, learning_rate : 0.3, max_depth : 3, reg_alpha : 0.3\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000183 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Train Confusion Matrix\n","[[228  13]\n"," [ 19 112]]\n","Train Acc : 0.9139784946236559\n","Train F1-Score : 0.875\n","Test Confusion Matrix\n","[[96 14]\n"," [ 1 49]]\n","TesT Acc : 0.90625\n","Test F1-Score : 0.8672566371681417\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 8 <<<\n","n_estimators : 5, learning_rate : 0.3, max_depth : 3, reg_alpha : 0.5\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000145 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Train Confusion Matrix\n","[[228  13]\n"," [ 17 114]]\n","Train Acc : 0.9193548387096774\n","Train F1-Score : 0.883720930232558\n","Test Confusion Matrix\n","[[96 14]\n"," [ 1 49]]\n","TesT Acc : 0.90625\n","Test F1-Score : 0.8672566371681417\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 9 <<<\n","n_estimators : 5, learning_rate : 0.3, max_depth : 5, reg_alpha : 0.1\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000098 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Train Confusion Matrix\n","[[232   9]\n"," [ 20 111]]\n","Train Acc : 0.9220430107526881\n","Train F1-Score : 0.8844621513944223\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Test Confusion Matrix\n","[[95 15]\n"," [ 4 46]]\n","TesT Acc : 0.88125\n","Test F1-Score : 0.8288288288288288\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 10 <<<\n","n_estimators : 5, learning_rate : 0.3, max_depth : 5, reg_alpha : 0.3\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000187 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Train Confusion Matrix\n","[[232   9]\n"," [ 17 114]]\n","Train Acc : 0.9301075268817204\n","Train F1-Score : 0.8976377952755905\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Test Confusion Matrix\n","[[94 16]\n"," [ 5 45]]\n","TesT Acc : 0.86875\n","Test F1-Score : 0.8108108108108109\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 11 <<<\n","n_estimators : 5, learning_rate : 0.3, max_depth : 5, reg_alpha : 0.5\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000093 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Train Confusion Matrix\n","[[231  10]\n"," [ 21 110]]\n","Train Acc : 0.9166666666666666\n","Train F1-Score : 0.8764940239043825\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Test Confusion Matrix\n","[[97 13]\n"," [ 3 47]]\n","TesT Acc : 0.9\n","Test F1-Score : 0.8545454545454546\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 12 <<<\n","n_estimators : 10, learning_rate : 0.1, max_depth : 3, reg_alpha : 0.1\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000095 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Train Confusion Matrix\n","[[233   8]\n"," [ 36  95]]\n","Train Acc : 0.8817204301075269\n","Train F1-Score : 0.811965811965812\n","Test Confusion Matrix\n","[[98 12]\n"," [ 4 46]]\n","TesT Acc : 0.9\n","Test F1-Score : 0.851851851851852\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 13 <<<\n","n_estimators : 10, learning_rate : 0.1, max_depth : 3, reg_alpha : 0.3\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000090 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Train Confusion Matrix\n","[[233   8]\n"," [ 36  95]]\n","Train Acc : 0.8817204301075269\n","Train F1-Score : 0.811965811965812\n","Test Confusion Matrix\n","[[98 12]\n"," [ 4 46]]\n","TesT Acc : 0.9\n","Test F1-Score : 0.851851851851852\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 14 <<<\n","n_estimators : 10, learning_rate : 0.1, max_depth : 3, reg_alpha : 0.5\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000189 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Train Confusion Matrix\n","[[233   8]\n"," [ 33  98]]\n","Train Acc : 0.8897849462365591\n","Train F1-Score : 0.8270042194092826\n","Test Confusion Matrix\n","[[98 12]\n"," [ 5 45]]\n","TesT Acc : 0.89375\n","Test F1-Score : 0.8411214953271027\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 15 <<<\n","n_estimators : 10, learning_rate : 0.1, max_depth : 5, reg_alpha : 0.1\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000155 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Train Confusion Matrix\n","[[232   9]\n"," [ 28 103]]\n","Train Acc : 0.9005376344086021\n","Train F1-Score : 0.8477366255144032\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Test Confusion Matrix\n","[[97 13]\n"," [ 4 46]]\n","TesT Acc : 0.89375\n","Test F1-Score : 0.8440366972477064\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 16 <<<\n","n_estimators : 10, learning_rate : 0.1, max_depth : 5, reg_alpha : 0.3\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000177 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Train Confusion Matrix\n","[[233   8]\n"," [ 25 106]]\n","Train Acc : 0.9112903225806451\n","Train F1-Score : 0.8653061224489796\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Test Confusion Matrix\n","[[96 14]\n"," [ 4 46]]\n","TesT Acc : 0.8875\n","Test F1-Score : 0.8363636363636363\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 17 <<<\n","n_estimators : 10, learning_rate : 0.1, max_depth : 5, reg_alpha : 0.5\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000109 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Train Confusion Matrix\n","[[232   9]\n"," [ 28 103]]\n","Train Acc : 0.9005376344086021\n","Train F1-Score : 0.8477366255144032\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Test Confusion Matrix\n","[[97 13]\n"," [ 4 46]]\n","TesT Acc : 0.89375\n","Test F1-Score : 0.8440366972477064\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 18 <<<\n","n_estimators : 10, learning_rate : 0.3, max_depth : 3, reg_alpha : 0.1\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000227 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Train Confusion Matrix\n","[[227  14]\n"," [ 12 119]]\n","Train Acc : 0.9301075268817204\n","Train F1-Score : 0.9015151515151515\n","Test Confusion Matrix\n","[[92 18]\n"," [ 1 49]]\n","TesT Acc : 0.88125\n","Test F1-Score : 0.8376068376068375\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 19 <<<\n","n_estimators : 10, learning_rate : 0.3, max_depth : 3, reg_alpha : 0.3\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000109 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Train Confusion Matrix\n","[[229  12]\n"," [ 10 121]]\n","Train Acc : 0.9408602150537635\n","Train F1-Score : 0.9166666666666667\n","Test Confusion Matrix\n","[[92 18]\n"," [ 1 49]]\n","TesT Acc : 0.88125\n","Test F1-Score : 0.8376068376068375\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 20 <<<\n","n_estimators : 10, learning_rate : 0.3, max_depth : 3, reg_alpha : 0.5\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000079 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Train Confusion Matrix\n","[[229  12]\n"," [ 13 118]]\n","Train Acc : 0.9327956989247311\n","Train F1-Score : 0.9042145593869731\n","Test Confusion Matrix\n","[[92 18]\n"," [ 1 49]]\n","TesT Acc : 0.88125\n","Test F1-Score : 0.8376068376068375\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 21 <<<\n","n_estimators : 10, learning_rate : 0.3, max_depth : 5, reg_alpha : 0.1\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000092 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Train Confusion Matrix\n","[[233   8]\n"," [ 12 119]]\n","Train Acc : 0.946236559139785\n","Train F1-Score : 0.9224806201550387\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Test Confusion Matrix\n","[[98 12]\n"," [ 3 47]]\n","TesT Acc : 0.90625\n","Test F1-Score : 0.8623853211009174\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 22 <<<\n","n_estimators : 10, learning_rate : 0.3, max_depth : 5, reg_alpha : 0.3\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000179 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Train Confusion Matrix\n","[[234   7]\n"," [ 10 121]]\n","Train Acc : 0.9543010752688172\n","Train F1-Score : 0.9343629343629344\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Test Confusion Matrix\n","[[96 14]\n"," [ 4 46]]\n","TesT Acc : 0.8875\n","Test F1-Score : 0.8363636363636363\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 23 <<<\n","n_estimators : 10, learning_rate : 0.3, max_depth : 5, reg_alpha : 0.5\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000086 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Train Confusion Matrix\n","[[234   7]\n"," [ 11 120]]\n","Train Acc : 0.9516129032258065\n","Train F1-Score : 0.9302325581395349\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Test Confusion Matrix\n","[[96 14]\n"," [ 4 46]]\n","TesT Acc : 0.8875\n","Test F1-Score : 0.8363636363636363\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 24 <<<\n","n_estimators : 20, learning_rate : 0.1, max_depth : 3, reg_alpha : 0.1\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000144 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Train Confusion Matrix\n","[[229  12]\n"," [ 17 114]]\n","Train Acc : 0.9220430107526881\n","Train F1-Score : 0.8871595330739299\n","Test Confusion Matrix\n","[[95 15]\n"," [ 2 48]]\n","TesT Acc : 0.89375\n","Test F1-Score : 0.8495575221238937\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 25 <<<\n","n_estimators : 20, learning_rate : 0.1, max_depth : 3, reg_alpha : 0.3\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000155 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Train Confusion Matrix\n","[[229  12]\n"," [ 14 117]]\n","Train Acc : 0.9301075268817204\n","Train F1-Score : 0.9\n","Test Confusion Matrix\n","[[94 16]\n"," [ 1 49]]\n","TesT Acc : 0.89375\n","Test F1-Score : 0.8521739130434782\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 26 <<<\n","n_estimators : 20, learning_rate : 0.1, max_depth : 3, reg_alpha : 0.5\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000077 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Train Confusion Matrix\n","[[228  13]\n"," [ 13 118]]\n","Train Acc : 0.9301075268817204\n","Train F1-Score : 0.9007633587786259\n","Test Confusion Matrix\n","[[94 16]\n"," [ 1 49]]\n","TesT Acc : 0.89375\n","Test F1-Score : 0.8521739130434782\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 27 <<<\n","n_estimators : 20, learning_rate : 0.1, max_depth : 5, reg_alpha : 0.1\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000159 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Train Confusion Matrix\n","[[233   8]\n"," [ 15 116]]\n","Train Acc : 0.9381720430107527\n","Train F1-Score : 0.9098039215686274\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Test Confusion Matrix\n","[[95 15]\n"," [ 3 47]]\n","TesT Acc : 0.8875\n","Test F1-Score : 0.8392857142857143\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 28 <<<\n","n_estimators : 20, learning_rate : 0.1, max_depth : 5, reg_alpha : 0.3\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000169 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Train Confusion Matrix\n","[[232   9]\n"," [ 14 117]]\n","Train Acc : 0.9381720430107527\n","Train F1-Score : 0.9105058365758756\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Test Confusion Matrix\n","[[95 15]\n"," [ 2 48]]\n","TesT Acc : 0.89375\n","Test F1-Score : 0.8495575221238937\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 29 <<<\n","n_estimators : 20, learning_rate : 0.1, max_depth : 5, reg_alpha : 0.5\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000609 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Train Confusion Matrix\n","[[232   9]\n"," [ 17 114]]\n","Train Acc : 0.9301075268817204\n","Train F1-Score : 0.8976377952755905\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Test Confusion Matrix\n","[[96 14]\n"," [ 3 47]]\n","TesT Acc : 0.89375\n","Test F1-Score : 0.8468468468468469\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 30 <<<\n","n_estimators : 20, learning_rate : 0.3, max_depth : 3, reg_alpha : 0.1\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000194 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Train Confusion Matrix\n","[[236   5]\n"," [  6 125]]\n","Train Acc : 0.9704301075268817\n","Train F1-Score : 0.9578544061302683\n","Test Confusion Matrix\n","[[89 21]\n"," [ 1 49]]\n","TesT Acc : 0.8625\n","Test F1-Score : 0.8166666666666667\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 31 <<<\n","n_estimators : 20, learning_rate : 0.3, max_depth : 3, reg_alpha : 0.3\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000097 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Train Confusion Matrix\n","[[233   8]\n"," [  6 125]]\n","Train Acc : 0.9623655913978495\n","Train F1-Score : 0.9469696969696969\n","Test Confusion Matrix\n","[[91 19]\n"," [ 1 49]]\n","TesT Acc : 0.875\n","Test F1-Score : 0.8305084745762712\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 32 <<<\n","n_estimators : 20, learning_rate : 0.3, max_depth : 3, reg_alpha : 0.5\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000180 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Train Confusion Matrix\n","[[234   7]\n"," [  8 123]]\n","Train Acc : 0.9596774193548387\n","Train F1-Score : 0.9425287356321839\n","Test Confusion Matrix\n","[[93 17]\n"," [ 1 49]]\n","TesT Acc : 0.8875\n","Test F1-Score : 0.8448275862068965\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 33 <<<\n","n_estimators : 20, learning_rate : 0.3, max_depth : 5, reg_alpha : 0.1\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000090 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Train Confusion Matrix\n","[[239   2]\n"," [  2 129]]\n","Train Acc : 0.989247311827957\n","Train F1-Score : 0.9847328244274809\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Test Confusion Matrix\n","[[94 16]\n"," [ 2 48]]\n","TesT Acc : 0.8875\n","Test F1-Score : 0.8421052631578947\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 34 <<<\n","n_estimators : 20, learning_rate : 0.3, max_depth : 5, reg_alpha : 0.3\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000153 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Train Confusion Matrix\n","[[237   4]\n"," [  0 131]]\n","Train Acc : 0.989247311827957\n","Train F1-Score : 0.9849624060150376\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Test Confusion Matrix\n","[[93 17]\n"," [ 2 48]]\n","TesT Acc : 0.88125\n","Test F1-Score : 0.8347826086956522\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n",">>> 35 <<<\n","n_estimators : 20, learning_rate : 0.3, max_depth : 5, reg_alpha : 0.5\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000096 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Train Confusion Matrix\n","[[237   4]\n"," [  1 130]]\n","Train Acc : 0.9865591397849462\n","Train F1-Score : 0.9811320754716981\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","Test Confusion Matrix\n","[[92 18]\n"," [ 4 46]]\n","TesT Acc : 0.8625\n","Test F1-Score : 0.8070175438596492\n","-----------------------------------------------------------------------\n","-----------------------------------------------------------------------\n"]}],"source":["# n_estimators\n","n_tree = [5, 10, 20]\n","# learning_rate\n","l_rate = [0.1, 0.3]\n","# max_depth\n","m_depth = [3, 5]\n","# reg_alpha\n","L1_norm = [0.1, 0.3, 0.5]\n","\n","# Modeling\n","save_n = []\n","save_l = []\n","save_m = []\n","save_L1 = []\n","f1_score_ = []\n","\n","cnt = 0\n","\n","for n in n_tree:\n","    for l in l_rate:\n","        for m in m_depth:\n","            for L1 in L1_norm:\n","                \n","                print(\">>> {} <<<\".format(cnt))\n","                cnt +=1\n","                print(\"n_estimators : {}, learning_rate : {}, max_depth : {}, reg_alpha : {}\".format(n, l, m, L1))\n","                model = LGBMClassifier(n_estimators=n, learning_rate=l, \n","                                       max_depth=m, reg_alpha=L1, \n","                                       n_jobs=-1, objective='cross_entropy')\n","                model.fit(X.iloc[train_idx], Y.iloc[train_idx])\n","                \n","                \n","                # Train Acc\n","                y_pre_train = model.predict(X.iloc[train_idx])\n","                cm_train = confusion_matrix(Y.iloc[train_idx], y_pre_train)\n","                print(\"Train Confusion Matrix\")\n","                print(cm_train)\n","                print(\"Train Acc : {}\".format((cm_train[0,0] + cm_train[1,1])/cm_train.sum()))\n","                print(\"Train F1-Score : {}\".format(f1_score(Y.iloc[train_idx], y_pre_train)))\n","\n","                # Test Acc\n","                y_pre_test = model.predict(X.iloc[valid_idx])\n","                cm_test = confusion_matrix(Y.iloc[valid_idx], y_pre_test)\n","                print(\"Test Confusion Matrix\")\n","                print(cm_test)\n","                print(\"TesT Acc : {}\".format((cm_test[0,0] + cm_test[1,1])/cm_test.sum()))\n","                print(\"Test F1-Score : {}\".format(f1_score(Y.iloc[valid_idx], y_pre_test)))\n","                print(\"-----------------------------------------------------------------------\")\n","                print(\"-----------------------------------------------------------------------\")\n","                save_n.append(n)\n","                save_l.append(l)\n","                save_m.append(m)\n","                save_L1.append(L1)\n","                f1_score_.append(f1_score(Y.iloc[valid_idx], y_pre_test))\n","\n","\n","                #joblib.dump(model, './LightGBM_model/Result_{}_{}_{}_{}_{}.pkl'.format(n, l, m, L1, round(save_acc[-1], 4)))\n","                #gc.collect()"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":579,"status":"ok","timestamp":1677304026710,"user":{"displayName":"ÏïàÍ±¥Ïù¥","userId":"00323974519415085515"},"user_tz":-540},"id":"kSwDemERG4k3","outputId":"f151e7db-6eaa-4c9d-edbe-fcfd67060ed9"},"outputs":[{"name":"stdout","output_type":"stream","text":[">>> 7 <<<\n","Best Test f1-score : 0.8672566371681417\n","Best n_estimators : 5\n","Best Learning Rate : 0.3\n","Best Max_depth : 3\n","Best L1-norm : 0.3\n"]}],"source":["print(\">>> {} <<<\\nBest Test f1-score : {}\\nBest n_estimators : {}\\nBest Learning Rate : {}\\nBest Max_depth : {}\\nBest L1-norm : {}\".format(np.argmax(f1_score_),\n","                                                                                                                                            f1_score_[np.argmax(f1_score_)], \n","                                                                                                                                            save_n[np.argmax(f1_score_)],\n","                                                                                                                                            save_l[np.argmax(f1_score_)],\n","                                                                                                                                            save_m[np.argmax(f1_score_)],\n","                                                                                                                                            save_L1[np.argmax(f1_score_)]))"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":289,"status":"ok","timestamp":1677304166877,"user":{"displayName":"ÏïàÍ±¥Ïù¥","userId":"00323974519415085515"},"user_tz":-540},"id":"uxOZPKUaEzfC","outputId":"4a6db841-c543-430e-ddd4-3ad06f0b7a35"},"outputs":[{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n","[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000194 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 905\n","[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n","[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n","[LightGBM] [Info] Start training from score -0.609600\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Train Confusion Matrix\n","[[228  13]\n"," [ 19 112]]\n","Train Acc : 0.9139784946236559\n","Train F1-Score : 0.875\n","Test Confusion Matrix\n","[[96 14]\n"," [ 1 49]]\n","TesT Acc : 0.90625\n","Test F1-Score : 0.8672566371681417\n"]}],"source":["best_model = LGBMClassifier(n_estimators=save_n[np.argmax(f1_score_)], learning_rate=save_l[np.argmax(f1_score_)], \n","                           max_depth=save_m[np.argmax(f1_score_)], reg_alpha=save_L1[np.argmax(f1_score_)], objective='cross_entropy', \n","                           random_state=119)\n","best_model.fit(X.iloc[train_idx], Y.iloc[train_idx])\n","\n","# Train Acc\n","y_pre_train = best_model.predict(X.iloc[train_idx])\n","cm_train = confusion_matrix(Y.iloc[train_idx], y_pre_train)\n","print(\"Train Confusion Matrix\")\n","print(cm_train)\n","print(\"Train Acc : {}\".format((cm_train[0,0] + cm_train[1,1])/cm_train.sum()))\n","print(\"Train F1-Score : {}\".format(f1_score(Y.iloc[train_idx], y_pre_train)))\n","\n","# Test Acc\n","y_pre_test = best_model.predict(X.iloc[valid_idx])\n","cm_test = confusion_matrix(Y.iloc[valid_idx], y_pre_test)\n","print(\"Test Confusion Matrix\")\n","print(cm_test)\n","print(\"TesT Acc : {}\".format((cm_test[0,0] + cm_test[1,1])/cm_test.sum()))\n","print(\"Test F1-Score : {}\".format(f1_score(Y.iloc[valid_idx], y_pre_test)))"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":343,"status":"ok","timestamp":1677304174236,"user":{"displayName":"ÏïàÍ±¥Ïù¥","userId":"00323974519415085515"},"user_tz":-540},"id":"GUl8n94_HEIL","outputId":"f11d41dc-dd3d-42ce-eff1-5cc033cb7ff7"},"outputs":[{"name":"stdout","output_type":"stream","text":["    Score  Feature\n","0      10    event\n","1       4    cd496\n","2       4    cd420\n","3       4     cd40\n","4       3  preanti\n","5       2     wtkg\n","6       2     race\n","7       1      z30\n","8       1      age\n","9       0  symptom\n","10      0    strat\n","11      0     str2\n","12      0        r\n","13      0   oprior\n","14      0   karnof\n","15      0     homo\n","16      0     hemo\n","17      0   gender\n","18      0    drugs\n","19      0    cd820\n","20      0     cd80\n"]}],"source":["feature_map = pd.DataFrame(sorted(zip(best_model.feature_importances_, X.columns), reverse=True), columns=['Score', 'Feature'])\n","print(feature_map)"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":720},"executionInfo":{"elapsed":778,"status":"ok","timestamp":1677304178102,"user":{"displayName":"ÏïàÍ±¥Ïù¥","userId":"00323974519415085515"},"user_tz":-540},"id":"Zup8IF1zHHCN","outputId":"7103bb58-2c67-491a-e3ba-4fd26aa350f1"},"outputs":[{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAB8YAAAPdCAYAAAD4WQIbAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAB14klEQVR4nOzdZ5RV9b3/8c/AMDO0GfQKWECxRtCgBKJBMGDQoFdNM4n1il0UYi/hqtiSYBevXkVjjCVqYkksscUGCileUUhiFBuoib3NgOigzPk/8M9JJiNKdXTzeq111uLs89v7fPeZecJ6z96nolQqlQIAAAAAAAAABdWmtQcAAAAAAAAAgOVJGAcAAAAAAACg0IRxAAAAAAAAAApNGAcAAAAAAACg0IRxAAAAAAAAAApNGAcAAAAAAACg0IRxAAAAAAAAAApNGAcAAAAAAACg0IRxAAAAAAAAAApNGAcAAAAAAACg0IRxAAAAltoee+yRmpqaPPnkky1eO+2001JRUZHf/va3zbY3Njbm/PPPz+DBg7PSSiulqqoqq6++er7xjW/k2muvzfz588trZ82alYqKimaP2trabLrpprnggguarW0tF154YS6//PJFXl9RUZHRo0cvv4GWs9///vc56aST8vbbb7f2KMvEXnvt1eJ3bMHjzjvvXC7vec0112T8+PHL5dgAAAA0V9naAwAAAPD5d8455+T222/PyJEjc99995W3z5w5M6ecckp22mmn7LDDDuXtr732WrbbbrtMnTo1w4cPz/HHH5+VV145L7/8cu65557stttuefrpp3PCCSc0e59dd901//mf/5kkqa+vz+23354f/OAHee6553LmmWd+Oie7EBdeeGFWWWWV7LXXXq06x6fl97//fU4++eTstdde6dKlS2uPs0xUV1fn0ksvbbF9k002WS7vd8011+Svf/1rDjvssOVyfAAAAP5JGAcAAGCpdevWLaeffnoOOOCAXHHFFRkxYkSS5OCDD067du1y3nnnNVv/X//1X3n00Udz44035jvf+U6z18aMGZOHH344M2bMaPE+X/rSl7LHHnuUnx988MHZfPPNc80117R6GF9RvPPOO+nYsWNrj7FcVFZWNvv9+ryaO3duOnTo0NpjAAAAfKa4lToAAADLxH777ZdBgwblqKOOyhtvvJFf/vKXufPOO/OjH/0oa6yxRnndH/7wh9x111054IADWkTxBQYMGJDdd9/9E9+zoqIi3bt3T2Vly7/7vvDCC7PRRhuluro6q6++ekaNGvWRt/2+/vrr079//7Rv3z6rrLJK9thjj/zjH/9otubll1/O3nvvnR49eqS6ujqrrbZavvnNb2bWrFlJkl69euWxxx7LpEmTyrffHjp06CfO/68mTpyYioqKXHfddTn55JOzxhprpHPnzvnud7+b+vr6NDY25rDDDku3bt3SqVOn7L333mlsbGzxeYwePTpXX311vvCFL6Smpib9+/fPAw880OL9Hn300Wy33Xapra1Np06dMmzYsPzxj39stubyyy9PRUVFJk2alIMPPjjdunVLjx49ctJJJ+Xoo49Okqy99trlc17wefz85z/P1772tXTr1i3V1dXp06dPLrroohYz9OrVKzvssEMmT56czTbbLDU1NVlnnXVy5ZVXtlj79ttv5/DDD0+vXr1SXV2dHj16ZM8998zrr79eXtPY2JgTTzwx6623Xqqrq9OzZ88cc8wxLT6nJdXU1JTx48dno402Sk1NTbp3754DDzwwb731VrN1N998c7bffvusvvrqqa6uzrrrrptTTz212S3/hw4dmttuuy3PPfdc+fPr1atXkn9+7gs+zwUW/I5MnDix2XE23njjTJ06NV/96lfToUOH/Pd///difR533313Bg8enC5duqRTp075whe+UD4GAABAUbhiHAAAgGWioqIiF198cfr165eDDjooDz74YAYMGJBRo0Y1W3frrbcmyRJdmTt37txyCG1oaMgdd9yRO++8M2PGjGm27qSTTsrJJ5+crbfeOgcddFBmzJiRiy66KP/3f/+XKVOmpF27dkk+DJB77713vvzlL2fcuHF55ZVXct5552XKlCl59NFHy7cI32mnnfLYY4/lBz/4QXr16pVXX301d999d55//vn06tUr48ePzw9+8IN06tQpxx13XJKke/fui31+STJu3Li0b98+P/zhD/P000/n/PPPT7t27dKmTZu89dZbOemkk/LHP/4xl19+edZee+2MHTu22f6TJk3Kr371qxxyyCGprq7OhRdemG233TYPPfRQNt544yTJY489li233DK1tbU55phj0q5du1x88cUZOnRoJk2alM0337zZMQ8++OB07do1Y8eOzTvvvJPtttsuTz75ZK699tqce+65WWWVVZIkXbt2TZJcdNFF2WijjfKNb3wjlZWVufXWW3PwwQenqampxe/D008/ne9+97vZd999M2LEiFx22WXZa6+90r9//2y00UZJkjlz5mTLLbfM448/nn322Sdf+tKX8vrrr+eWW27J3//+96yyyippamrKN77xjUyePDkHHHBAevfunb/85S8599xz8+STT+amm25apM//X0N7krRr1y51dXVJkgMPPLD8O3PIIYdk5syZueCCC/Loo4+2+L3q1KlTjjjiiHTq1Cn33Xdfxo4dm4aGhvKdDY477rjU19fn73//e84999wkSadOnRZpxn/3xhtvZLvttssuu+ySPfbYI927d1/kz+Oxxx7LDjvskL59++aUU05JdXV1nn766UyZMmWJZgEAAPjMKgEAAMAyNGbMmFKSUtu2bUtTp05t8fq3v/3tUpLS22+/3Wz7u+++W3rttdfKj7feeqv82syZM0tJPvJx0EEHlZqamsprX3311VJVVVXp61//emn+/Pnl7RdccEEpSemyyy4rlUql0rx580rdunUrbbzxxqV33323vO63v/1tKUlp7NixpVKpVHrrrbdKSUpnnnnmx573RhttVBoyZMgif05JSqNGjSo/v//++0tJShtvvHFp3rx55e277rprqaKiorTddts123/gwIGltdZaq8Uxk5Qefvjh8rbnnnuuVFNTU/r2t79d3vatb32rVFVVVXrmmWfK21588cVS586dS1/96lfL237+85+XkpQGDx5c+uCDD5q915lnnllKUpo5c2aLc5s7d26LbcOHDy+ts846zbattdZapSSlBx54oLzt1VdfLVVXV5eOPPLI8raxY8eWkpR+/etftzjugp/9VVddVWrTpk3pwQcfbPb6hAkTSklKU6ZMabHvvxoxYsRH/n4t+Jk++OCDpSSlq6++utl+d955Z4vtH3X+Bx54YKlDhw6l9957r7xt++23b/EzLJX++bn/+2e74Hfk/vvvL28bMmRIKUlpwoQJzdYu6udx7rnnlpKUXnvttYV+NgAAAEXgVuoAAAAsUwuuHl599dXLVyj/q4aGhiQtr46dMGFCunbtWn4MHjy4xb4HHHBA7r777tx999258cYbM2rUqFx88cU54ogjymvuueeezJs3L4cddljatPnnf3v333//1NbW5rbbbkuSPPzww3n11Vdz8MEHp6amprxu++23z4Ybblhe1759+1RVVWXixIktbpm9POy5557lK4+TZPPNN0+pVMo+++zTbN3mm2+eF154IR988EGz7QMHDkz//v3Lz9dcc81885vfzF133ZX58+dn/vz5+d3vfpdvfetbWWeddcrrVltttey2226ZPHly+We0wP7775+2bdsu8jm0b9++/O/6+vq8/vrrGTJkSJ599tnU19c3W9unT59sueWW5eddu3bNF77whTz77LPlbTfeeGM22WSTfPvb327xXhUVFUk+vCV+7969s+GGG+b1118vP772ta8lSe6///5PnLumpqb8+7XgcfbZZ5ePX1dXl2222abZ8fv3759OnTo1O/6/nv/s2bPz+uuvZ8stt8zcuXPzxBNPfOIci6u6ujp77713s22L+nksuCvCzTffnKampmU+GwAAwGeFW6kDAACwzLzwwgs58cQTs/HGG+evf/1rzjjjjBx//PHN1nTu3DnJh7fHXnCL6uTD25UvCOlHHnlks+9jXmD99dfP1ltvXX7+ne98JxUVFRk/fnz22WeffPGLX8xzzz2XJPnCF77QbN+qqqqss8465dcXti5JNtxww0yePDnJh9Hx9NNPz5FHHpnu3bvnK1/5SnbYYYfsueeeWXXVVRfvA1oEa665ZrPnCz6jnj17ttje1NSU+vr6/Md//Ed5+/rrr9/imBtssEHmzp2b1157LcmHt6T/qPPu3bt3mpqa8sILL5RvY558+D3ii2PKlCk58cQT84c//CFz585t9lp9fX2zn/u/n2+SrLTSSs3+COGZZ57JTjvt9LHv+dRTT+Xxxx8v387937366qufOHfbtm2b/X79+/Hr6+vTrVu3Tzz+Y489luOPPz733Xdfiz8y+Pc/DFgW1lhjjVRVVbWYd1E+j5133jmXXnpp9ttvv/zwhz/MsGHD8p3vfCff/e53m/1hCQAAwOedMA4AAMAyM3r06CTJHXfckSOOOCI//vGPs9tuuzW7MnnDDTdMkvz1r3/NoEGDytt79uxZjr8rrbRSi+96Xphhw4blggsuyAMPPJAvfvGLy+pUmjnssMOy44475qabbspdd92VE044IePGjct9992Xfv36LdP3WtiV2QvbXiqVlun7f5R/vQL6kzzzzDMZNmxYNtxww5xzzjnp2bNnqqqqcvvtt+fcc89tcVXysjqvpqamfPGLX8w555zzka//+x8WLK6mpqZ069YtV1999Ue+viBAv/322xkyZEhqa2tzyimnZN11101NTU0eeeSRHHvssYt0VfaCq+D/3Uf9sUjy0T+fRf082rdvnwceeCD3339/brvtttx555351a9+la997Wv53e9+t1h3CgAAAPgsE8YBAABYJn7zm9/klltuybnnnpsePXpk/PjxueuuuzJq1Kjccccd5XU77LBDTjvttFx99dXNwviSWnAr8Tlz5iRJ1lprrSTJjBkzmgX5efPmZebMmeUrgv913YLbSy8wY8aM8usLrLvuujnyyCNz5JFH5qmnnsqmm26as88+O7/4xS+SLDxmftqeeuqpFtuefPLJdOjQoRxvO3TokBkzZrRY98QTT6RNmzaLFJEXdr633nprGhsbc8sttzS7GnxRbmW+MOuuu27++te/fuKa6dOnZ9iwYcvlZ7HuuuvmnnvuyaBBgz72DwUmTpyYN954I7/+9a/z1a9+tbx95syZLdYubM6VVlopyYeR/V8tuMvBos67qJ9HmzZtMmzYsAwbNiznnHNOfvKTn+S4447L/fffv9Ar6AEAAD5v3BMLAACApTZ79uwccsgh6devX37wgx8k+fA7xk899dTceeeduf7668trBw0alG222SaXXHJJbr755o883uJcLXzrrbcmSTbZZJMkydZbb52qqqr8z//8T7Pj/OxnP0t9fX223377JMmAAQPSrVu3TJgwIY2NjeV1d9xxRx5//PHyurlz5+a9995r9p7rrrtuOnfu3Gy/jh07tgiZreEPf/hDHnnkkfLzF154ITfffHO+/vWvp23btmnbtm2+/vWv5+abb86sWbPK61555ZVcc801GTx4cGpraz/xfTp27JikZbxdcIXxv3729fX1+fnPf77E57TTTjtl+vTp+c1vftPitQXv8/3vfz//+Mc/8tOf/rTFmnfffTfvvPPOEr//guPPnz8/p556aovXPvjgg/Ln8FHnP2/evFx44YUt9uvYseNH3lp93XXXTZI88MAD5W3z58/PJZdcsljzLsrn8eabb7Z4fdNNN02SZr/fAAAAn3euGAcAAGCpHX/88XnxxRfz61//utmtl0eNGpUrrrgihx12WLbddtvy94v/4he/yLbbbptvfetb2W677bL11ltnpZVWyssvv5x77rknDzzwQLbbbrsW7/PII4+Ur9CePXt27r333tx4443ZYost8vWvfz3Jh7e0HjNmTE4++eRsu+22+cY3vpEZM2bkwgsvzJe//OXsscceSZJ27drl9NNPz957750hQ4Zk1113zSuvvJLzzjsvvXr1yuGHH57kw6uthw0blu9///vp06dPKisr85vf/CavvPJKdtlll/Js/fv3z0UXXZQf/ehHWW+99dKtW7cWV6J/GjbeeOMMHz48hxxySKqrq8tB9uSTTy6v+dGPfpS77747gwcPzsEHH5zKyspcfPHFaWxszBlnnLFI79O/f/8kyXHHHZdddtkl7dq1y4477pivf/3rqaqqyo477pgDDzwwc+bMyU9/+tN069YtL7300hKd09FHH50bbrgh3/ve97LPPvukf//+efPNN3PLLbdkwoQJ2WSTTfJf//Vfue666zJy5Mjcf//9GTRoUObPn58nnngi1113Xe66664MGDBgid4/SYYMGZIDDzww48aNy7Rp0/L1r3897dq1y1NPPZXrr78+5513Xr773e9miy22yEorrZQRI0bkkEMOSUVFRa666qqP/GOP/v3751e/+lWOOOKIfPnLX06nTp2y4447ZqONNspXvvKVjBkzJm+++WZWXnnl/PKXvyzfHWFRLOrnccopp+SBBx7I9ttvn7XWWiuvvvpqLrzwwvTo0SODBw9e4s8LAADgM6cEAAAAS+Hhhx8utW3btjR69OiPfP2hhx4qtWnTpnTIIYc02/7uu++Wxo8fXxo4cGCptra2VFlZWVp11VVLO+ywQ+nqq68uffDBB+W1M2fOLCVp9qisrCyts846paOPPro0e/bsFu97wQUXlDbccMNSu3btSt27dy8ddNBBpbfeeqvFul/96lelfv36laqrq0srr7xyaffddy/9/e9/L7/++uuvl0aNGlXacMMNSx07dizV1dWVNt9889J1113X7Dgvv/xyafvtty917ty5lKQ0ZMiQj/3ckpRGjRpVfn7//feXkpSuv/76Zut+/vOfl5KU/u///q/Z9hNPPLGUpPTaa6+1OOYvfvGL0vrrr1+qrq4u9evXr3T//fe3eP9HHnmkNHz48FKnTp1KHTp0KG211Val3//+94v03guceuqppTXWWKPUpk2bUpLSzJkzS6VSqXTLLbeU+vbtW6qpqSn16tWrdPrpp5cuu+yyZmtKpVJprbXWKm2//fYtjjtkyJAWn98bb7xRGj16dGmNNdYoVVVVlXr06FEaMWJE6fXXXy+vmTdvXun0008vbbTRRqXq6urSSiutVOrfv3/p5JNPLtXX13/kOSwwYsSIUseOHT92TalUKl1yySWl/v37l9q3b1/q3Llz6Ytf/GLpmGOOKb344ovlNVOmTCl95StfKbVv3760+uqrl4455pjSXXfdVUrS7GcxZ86c0m677Vbq0qVLKUlprbXWKr/2zDPPlLbeeutSdXV1qXv37qX//u//Lt19990tjjFkyJDSRhtt9JGzLsrnce+995a++c1vllZfffVSVVVVafXVVy/tuuuupSeffPITPwsAAIDPk4pSaTHuTwcAAAB8ZlVUVGTUqFG54IILWnsUAAAA+EzxHeMAAAAAAAAAFJowDgAAAAAAAEChCeMAAAAAAAAAFFplaw8AAAAALBulUqm1RwAAAIDPJFeMAwAAAAAAAFBorhj/jGtqasqLL76Yzp07p6KiorXHAQAAAAAAAPhMKJVKmT17dlZfffW0afPx14QL459xL774Ynr27NnaYwAAAAAAAAB8Jr3wwgvp0aPHx64Rxj/jOnfunOTDH2ZtbW0rTwMAAAAAAADw2dDQ0JCePXuWm+rHEcY/4xbcPr22tlYYBwAAAAAAAPg3i/KV1B9/o3UAAAAAAAAA+JwTxgEAAAAAAAAoNGEcAAAAAAAAgEITxgEAAAAAAAAoNGEcAAAAAAAAgEITxgEAAAAAAAAotMrWHoBF89Xjr03b6vatPQYAAAAAAACwiKaeuWdrj8D/54pxAAAAAAAAAApNGAcAAAAAAACg0IRxAAAAAAAAAApNGAcAAAAAAACg0IRxAAAAAAAAAApNGAcAAAAAAACg0IRxAAAAAAAAAApNGAcAAAAAAACg0IRxAAAAAAAAAApNGAcAAAAAAACg0IRxAAAAAAAAAApNGAcAAAAAAACg0IRxAAAAAAAAAApNGAcAAAAAAACg0IRxAAAAAAAAAApNGAcAAAAAAACg0IRxAAAAAAAAAApNGAcAAAAAAACg0IRxAAAAAAAAAApNGAcAAAAAAACg0IRxAAAAAAAAAApNGAcAAAAAAACg0IRxAAAAAAAAAApNGAcAAAAAAACg0IRxAAAAAAAAAApNGAcAAAAAAACg0IRxAAAAAAAAAApNGAcAAAAAAACg0IRxAAAAAAAAAApNGAcAAAAAAACg0IRxAAAAAAAAAApNGAcAAAAAAACg0ITxT8nQoUNz2GGHtfYYAAAAAAAAACscYRwAAAAAAACAQlshwnhTU1PGjRuXtddeO+3bt88mm2ySG264IU1NTenRo0cuuuiiZusfffTRtGnTJs8991yS5O23385+++2Xrl27pra2Nl/72tcyffr08vqTTjopm266aa666qr06tUrdXV12WWXXTJ79uwkyV577ZVJkyblvPPOS0VFRSoqKjJr1qxP7fwBAAAAAAAAVmQrRBgfN25crrzyykyYMCGPPfZYDj/88Oyxxx558MEHs+uuu+aaa65ptv7qq6/OoEGDstZaayVJvve97+XVV1/NHXfckalTp+ZLX/pShg0bljfffLO8zzPPPJObbropv/3tb/Pb3/42kyZNymmnnZYkOe+88zJw4MDsv//+eemll/LSSy+lZ8+eHzlrY2NjGhoamj0AAAAAAAAAWHKFD+ONjY35yU9+kssuuyzDhw/POuusk7322it77LFHLr744uy+++6ZMmVKnn/++SQfXl3+y1/+MrvvvnuSZPLkyXnooYdy/fXXZ8CAAVl//fVz1llnpUuXLrnhhhvK79PU1JTLL788G2+8cbbccsv813/9V+69994kSV1dXaqqqtKhQ4esuuqqWXXVVdO2bduPnHfcuHGpq6srPxYW0AEAAAAAAABYNIUP408//XTmzp2bbbbZJp06dSo/rrzyyjzzzDPZdNNN07t37/JV45MmTcqrr76a733ve0mS6dOnZ86cOfmP//iPZvvPnDkzzzzzTPl9evXqlc6dO5efr7baann11VcXe94xY8akvr6+/HjhhReW8hMAAAAAAAAAWLFVtvYAy9ucOXOSJLfddlvWWGONZq9VV1cnSXbfffdcc801+eEPf5hrrrkm2267bf7jP/6jvP9qq62WiRMntjh2ly5dyv9u165ds9cqKirS1NS02PNWV1eX5wIAAAAAAABg6RU+jPfp0yfV1dV5/vnnM2TIkI9cs9tuu+X444/P1KlTc8MNN2TChAnl1770pS/l5ZdfTmVlZXr16rXEc1RVVWX+/PlLvD8AAAAAAAAAS6bwYbxz58456qijcvjhh6epqSmDBw9OfX19pkyZktra2owYMSK9evXKFltskX333Tfz58/PN77xjfL+W2+9dQYOHJhvfetbOeOMM7LBBhvkxRdfzG233ZZvf/vbGTBgwCLN0atXr/zpT3/KrFmz0qlTp6y88spp06bwd7IHAAAAAAAAaHUrRJk99dRTc8IJJ2TcuHHp3bt3tt1229x2221Ze+21y2t23333TJ8+Pd/+9rfTvn378vaKiorcfvvt+epXv5q99947G2ywQXbZZZc899xz6d69+yLPcNRRR6Vt27bp06dPunbtmueff36ZniMAAAAAAAAAH62iVCqVWnsIFq6hoSF1dXXZ5AcT0ra6/SfvAAAAAAAAAHwmTD1zz9YeodAWtNT6+vrU1tZ+7NoV4opxAAAAAAAAAFZcwjgAAAAAAAAAhSaMAwAAAAAAAFBowjgAAAAAAAAAhSaMAwAAAAAAAFBowjgAAAAAAAAAhSaMAwAAAAAAAFBowjgAAAAAAAAAhSaMAwAAAAAAAFBowjgAAAAAAAAAhSaMAwAAAAAAAFBowjgAAAAAAAAAhSaMAwAAAAAAAFBowjgAAAAAAAAAhSaMAwAAAAAAAFBowjgAAAAAAAAAhSaMAwAAAAAAAFBowjgAAAAAAAAAhSaMAwAAAAAAAFBowjgAAAAAAAAAhSaMAwAAAAAAAFBowjgAAAAAAAAAhSaMAwAAAAAAAFBowjgAAAAAAAAAhSaMAwAAAAAAAFBowjgAAAAAAAAAhSaMAwAAAAAAAFBowjgAAAAAAAAAhSaMAwAAAAAAAFBola09AIvmgR/tmtra2tYeAwAAAAAAAOBzxxXjAAAAAAAAABSaMA4AAAAAAABAoQnjAAAAAAAAABSaMA4AAAAAAABAoQnjAAAAAAAAABSaMA4AAAAAAABAoQnjAAAAAAAAABSaMA4AAAAAAABAoQnjAAAAAAAAABSaMA4AAAAAAABAoQnjAAAAAAAAABSaMA4AAAAAAABAoQnjAAAAAAAAABSaMA4AAAAAAABAoQnjAAAAAAAAABSaMA4AAAAAAABAoQnjAAAAAAAAABRaZWsPwKJ54bSvpHNN29YeAwAAPjPWHPuX1h4BAAAAgM8JV4wDAAAAAAAAUGjCOAAAAAAAAACFJowDAAAAAAAAUGjCOAAAAAAAAACFJowDAAAAAAAAUGjCOAAAAAAAAACFJowDAAAAAAAAUGjCOAAAAAAAAACFJowDAAAAAAAAUGjCOAAAAAAAAACFJowDAAAAAAAAUGjCOAAAAAAAAACFJowDAAAAAAAAUGjCOAAAAAAAAACFJowDAAAAAAAAUGjCOAAAAAAAAACFJowDAAAAAAAAUGjCOAAAAAAAAACFJowDAAAAAAAAUGjCOAAAAAAAAACFJowDAAAAAAAAUGjCOAAAAAAAAACFJowDAAAAAAAAUGjCOAAAAAAAAACFJowDAAAAAAAAUGjCOAAAAAAAAACFJowDAAAAAAAAUGjCOAAAAAAAAACFJowDAAAAAAAAUGjCOAAAAAAAAACFJownmTVrVioqKjJt2rTWHgUAAAAAAACAZUwY/wRvvPFGevTokYqKirz99tvNXvvf//3f9O7dO+3bt88XvvCFXHnllS32f/vttzNq1Kisttpqqa6uzgYbbJDbb7/9U5oeAAAAAAAAgMrWHuCzbt99903fvn3zj3/8o9n2iy66KGPGjMlPf/rTfPnLX85DDz2U/fffPyuttFJ23HHHJMm8efOyzTbbpFu3brnhhhuyxhpr5LnnnkuXLl1a4UwAAAAAAAAAVkyFvWK8qakpZ5xxRtZbb71UV1dnzTXXzI9//OMkyUMPPZR+/fqlpqYmAwYMyKOPPvqRx7jooovy9ttv56ijjmrx2lVXXZUDDzwwO++8c9ZZZ53ssssuOeCAA3L66aeX11x22WV58803c9NNN2XQoEHp1atXhgwZkk022WT5nDQAAAAAAAAALRT2ivEFV3Ofe+65GTx4cF566aU88cQTmTNnTnbYYYdss802+cUvfpGZM2fm0EMPbbH/3/72t5xyyin505/+lGeffbbF642NjampqWm2rX379nnooYfy/vvvp127drnlllsycODAjBo1KjfffHO6du2a3XbbLccee2zatm37kXM3NjamsbGx/LyhoWEpPwkAAAAAAACAFVshrxifPXt2zjvvvJxxxhkZMWJE1l133QwePDj77bdfrrnmmjQ1NeVnP/tZNtpoo+ywww45+uijm+3f2NiYXXfdNWeeeWbWXHPNj3yP4cOH59JLL83UqVNTKpXy8MMP59JLL83777+f119/PUny7LPP5oYbbsj8+fNz++2354QTTsjZZ5+dH/3oRwudfdy4camrqys/evbsuew+GAAAAAAAAIAVUCHD+OOPP57GxsYMGzbsI1/r27dvs6u9Bw4c2GzNmDFj0rt37+yxxx4LfY8TTjgh2223Xb7yla+kXbt2+eY3v5kRI0YkSdq0+fBjbWpqSrdu3XLJJZekf//+2XnnnXPcccdlwoQJCz3umDFjUl9fX3688MILi3XuAAAAAAAAADRXyDDevn37pdr/vvvuy/XXX5/KyspUVlaWA/sqq6ySE088sfwel112WebOnZtZs2bl+eefT69evdK5c+d07do1SbLaaqtlgw02aHbb9N69e+fll1/OvHnzPvK9q6urU1tb2+wBAAAAAAAAwJIrZBhff/310759+9x7770tXuvdu3f+/Oc/57333itv++Mf/9hszY033pjp06dn2rRpmTZtWi699NIkyYMPPphRo0Y1W9uuXbv06NEjbdu2zS9/+cvssMMO5SvGBw0alKeffjpNTU3l9U8++WRWW221VFVVLbPzBQAAAAAAAGDhKlt7gOWhpqYmxx57bI455phUVVVl0KBBee211/LYY49lt912y3HHHZf9998/Y8aMyaxZs3LWWWc123/ddddt9nzBd4b37t07Xbp0SfJh4H7ooYey+eab56233so555yTv/71r7niiivK+x100EG54IILcuihh+YHP/hBnnrqqfzkJz/JIYccsnw/AAAAAAAAAADKChnGkw+/A7yysjJjx47Niy++mNVWWy0jR45Mp06dcuutt2bkyJHp169f+vTpk9NPPz077bTTYh1//vz5OfvsszNjxoy0a9cuW221VX7/+9+nV69e5TU9e/bMXXfdlcMPPzx9+/bNGmuskUMPPTTHHnvsMj5bAAAAAAAAABamolQqlVp7CBauoaEhdXV1+euY3ulc0/aTdwAAgBXEmmP/0tojAAAAANCKFrTU+vr61NbWfuzaQn7HOAAAAAAAAAAsIIwDAAAAAAAAUGjCOAAAAAAAAACFJowDAAAAAAAAUGjCOAAAAAAAAACFJowDAAAAAAAAUGjCOAAAAAAAAACFJowDAAAAAAAAUGjCOAAAAAAAAACFJowDAAAAAAAAUGjCOAAAAAAAAACFJowDAAAAAAAAUGjCOAAAAAAAAACFJowDAAAAAAAAUGjCOAAAAAAAAACFJowDAAAAAAAAUGjCOAAAAAAAAACFJowDAAAAAAAAUGjCOAAAAAAAAACFJowDAAAAAAAAUGjCOAAAAAAAAACFJowDAAAAAAAAUGjCOAAAAAAAAACFJowDAAAAAAAAUGjCOAAAAAAAAACFJowDAAAAAAAAUGjCOAAAAAAAAACFJowDAAAAAAAAUGjCOAAAAAAAAACFVtnaA7Boev7wj6mtrW3tMQAAAAAAAAA+d1wxDgAAAAAAAEChCeMAAAAAAAAAFJowDgAAAAAAAEChCeMAAAAAAAAAFJowDgAAAAAAAEChCeMAAAAAAAAAFJowDgAAAAAAAEChCeMAAAAAAAAAFJowDgAAAAAAAEChCeMAAAAAAAAAFJowDgAAAAAAAEChCeMAAAAAAAAAFJowDgAAAAAAAEChCeMAAAAAAAAAFJowDgAAAAAAAEChCeMAAAAAAAAAFJowDgAAAAAAAEChVbb2ACyabSZsk8r2flwAALDAlB9Mae0RAAAAAPiccMU4AAAAAAAAAIUmjAMAAAAAAABQaMI4AAAAAAAAAIUmjAMAAAAAAABQaMI4AAAAAAAAAIUmjAMAAAAAAABQaMI4AAAAAAAAAIUmjAMAAAAAAABQaMI4AAAAAAAAAIUmjAMAAAAAAABQaMI4AAAAAAAAAIUmjAMAAAAAAABQaMI4AAAAAAAAAIUmjAMAAAAAAABQaMI4AAAAAAAAAIUmjAMAAAAAAABQaMI4AAAAAAAAAIUmjAMAAAAAAABQaMI4AAAAAAAAAIUmjAMAAAAAAABQaMI4AAAAAAAAAIUmjAMAAAAAAABQaMI4AAAAAAAAAIUmjAMAAAAAAABQaMI4AAAAAAAAAIUmjAMAAAAAAABQaMI4AAAAAAAAAIUmjAMAAAAAAABQaMI4AAAAAAAAAIUmjAMAAAAAAABQaMI4AAAAAAAAAIUmjCeZNWtWKioqMm3atNYeBQAAAAAAAIBlTBj/BG+88UZ69OiRioqKvP322+Xtv/71r7PNNtuka9euqa2tzcCBA3PXXXe12P9///d/06tXr9TU1GTzzTfPQw899ClODwAAAAAAAIAw/gn23Xff9O3bt8X2Bx54INtss01uv/32TJ06NVtttVV23HHHPProo+U1v/rVr3LEEUfkxBNPzCOPPJJNNtkkw4cPz6uvvvppngIAAAAAAADACq2wYbypqSlnnHFG1ltvvVRXV2fNNdfMj3/84yTJQw89lH79+qWmpiYDBgxoFrP/1UUXXZS33347Rx11VIvXxo8fn2OOOSZf/vKXs/766+cnP/lJ1l9//dx6663lNeecc07233//7L333unTp08mTJiQDh065LLLLlvo3I2NjWloaGj2AAAAAAAAAGDJFTaMjxkzJqeddlpOOOGE/O1vf8s111yT7t27Z86cOdlhhx3Sp0+fTJ06NSeddNJHhu+//e1vOeWUU3LllVemTZtP/piampoye/bsrLzyykmSefPmZerUqdl6663La9q0aZOtt946f/jDHxZ6nHHjxqWurq786Nmz5xKcPQAAAAAAAAALVLb2AMvD7Nmzc9555+WCCy7IiBEjkiTrrrtuBg8enEsuuSRNTU352c9+lpqammy00Ub5+9//noMOOqi8f2NjY3bdddeceeaZWXPNNfPss89+4nueddZZmTNnTr7//e8nSV5//fXMnz8/3bt3b7aue/fueeKJJxZ6nDFjxuSII44oP29oaBDHAQAAAAAAAJZCIcP4448/nsbGxgwbNuwjX+vbt29qamrK2wYOHNhszZgxY9K7d+/ssccei/R+11xzTU4++eTcfPPN6dat21LNXl1dnerq6qU6BgAAAAAAAAD/VMhbqbdv336p9r/vvvty/fXXp7KyMpWVleXAvsoqq+TEE09stvaXv/xl9ttvv1x33XXNbpu+yiqrpG3btnnllVearX/llVey6qqrLtV8AAAAAAAAACy6Qobx9ddfP+3bt8+9997b4rXevXvnz3/+c957773ytj/+8Y/N1tx4442ZPn16pk2blmnTpuXSSy9Nkjz44IMZNWpUed21116bvffeO9dee2223377ZseoqqpK//79m83Q1NSUe++9t8UV6gAAAAAAAAAsP4W8lXpNTU2OPfbYHHPMMamqqsqgQYPy2muv5bHHHstuu+2W4447Lvvvv3/GjBmTWbNm5ayzzmq2/7rrrtvs+euvv57kw6jepUuXJB/ePn3EiBE577zzsvnmm+fll19O8uHV6nV1dUmSI444IiNGjMiAAQOy2WabZfz48XnnnXey9957L+dPAAAAAAAAAIAFChnGk+SEE05IZWVlxo4dmxdffDGrrbZaRo4cmU6dOuXWW2/NyJEj069fv/Tp0yenn356dtppp8U6/iWXXJIPPvggo0aNanYV+YgRI3L55ZcnSXbeeee89tprGTt2bF5++eVsuummufPOO9O9e/dleaoAAAAAAAAAfIyKUqlUau0hWLiGhobU1dVls9M3S2X7wv4dAwAALLYpP5jS2iMAAAAA0IoWtNT6+vrU1tZ+7NpCfsc4AAAAAAAAACwgjAMAAAAAAABQaMI4AAAAAAAAAIUmjAMAAAAAAABQaMI4AAAAAAAAAIUmjAMAAAAAAABQaMI4AAAAAAAAAIUmjAMAAAAAAABQaMI4AAAAAAAAAIUmjAMAAAAAAABQaMI4AAAAAAAAAIUmjAMAAAAAAABQaMI4AAAAAAAAAIUmjAMAAAAAAABQaMI4AAAAAAAAAIUmjAMAAAAAAABQaMI4AAAAAAAAAIUmjAMAAAAAAABQaMI4AAAAAAAAAIUmjAMAAAAAAABQaMI4AAAAAAAAAIUmjAMAAAAAAABQaMI4AAAAAAAAAIUmjAMAAAAAAABQaMI4AAAAAAAAAIUmjAMAAAAAAABQaMI4AAAAAAAAAIUmjAMAAAAAAABQaMI4AAAAAAAAAIVW2doDsGjuHnl3amtrW3sMAAAAAAAAgM8dV4wDAAAAAAAAUGjCOAAAAAAAAACFJowDAAAAAAAAUGjCOAAAAAAAAACFJowDAAAAAAAAUGjCOAAAAAAAAACFJowDAAAAAAAAUGjCOAAAAAAAAACFJowDAAAAAAAAUGjCOAAAAAAAAACFJowDAAAAAAAAUGjCOAAAAAAAAACFJowDAAAAAAAAUGjCOAAAAAAAAACFJowDAAAAAAAAUGjCOAAAAAAAAACFJowDAAAAAAAAUGiVrT0Ai2byttulY6UfFwAALDDkgUmtPQIAAAAAnxOuGAcAAAAAAACg0IRxAAAAAAAAAApNGAcAAAAAAACg0IRxAAAAAAAAAApNGAcAAAAAAACg0IRxAAAAAAAAAApNGAcAAAAAAACg0IRxAAAAAAAAAApNGAcAAAAAAACg0IRxAAAAAAAAAApNGAcAAAAAAACg0IRxAAAAAAAAAApNGAcAAAAAAACg0IRxAAAAAAAAAApNGAcAAAAAAACg0IRxAAAAAAAAAApNGAcAAAAAAACg0IRxAAAAAAAAAApNGAcAAAAAAACg0IRxAAAAAAAAAApNGAcAAAAAAACg0IRxAAAAAAAAAApNGAcAAAAAAACg0IRxAAAAAAAAAApNGAcAAAAAAACg0IRxAAAAAAAAAApNGAcAAAAAAACg0IRxAAAAAAAAAApNGAcAAAAAAACg0IRxAAAAAAAAAApNGF9Es2bNSkVFRaZNm9baowAAAAAAAACwGITxZeCNN95Ijx49UlFRkbfffrvZaxMnTsyXvvSlVFdXZ7311svll1/eKjMCAAAAAAAArKiE8WVg3333Td++fVtsnzlzZrbffvtstdVWmTZtWg477LDst99+ueuuu1phSgAAAAAAAIAV0wodxpuamnLGGWdkvfXWS3V1ddZcc838+Mc/TpI89NBD6devX2pqajJgwIA8+uijH3mMiy66KG+//XaOOuqoFq9NmDAha6+9ds4+++z07t07o0ePzne/+92ce+65y/W8AAAAAAAAAPinytYeoDWNGTMmP/3pT3Puuedm8ODBeemll/LEE09kzpw52WGHHbLNNtvkF7/4RWbOnJlDDz20xf5/+9vfcsopp+RPf/pTnn322Rav/+EPf8jWW2/dbNvw4cNz2GGHLXSmxsbGNDY2lp83NDQs+QkCAAAAAAAAsOKG8dmzZ+e8887LBRdckBEjRiRJ1l133QwePDiXXHJJmpqa8rOf/Sw1NTXZaKON8ve//z0HHXRQef/GxsbsuuuuOfPMM7Pmmmt+ZBh/+eWX071792bbunfvnoaGhrz77rtp3759i33GjRuXk08+eRmfLQAAAAAAAMCKa4W9lfrjjz+exsbGDBs27CNf69u3b2pqasrbBg4c2GzNmDFj0rt37+yxxx7LdK4xY8akvr6+/HjhhReW6fEBAAAAAAAAVjQrbBj/qKu1F8d9992X66+/PpWVlamsrCwH9lVWWSUnnnhikmTVVVfNK6+80my/V155JbW1tQt9/+rq6tTW1jZ7AAAAAAAAALDkVtgwvv7666d9+/a59957W7zWu3fv/PnPf857771X3vbHP/6x2Zobb7wx06dPz7Rp0zJt2rRceumlSZIHH3wwo0aNSvLhVeb/fvy77767xdXnAAAAAAAAACw/K+x3jNfU1OTYY4/NMccck6qqqgwaNCivvfZaHnvssey222457rjjsv/++2fMmDGZNWtWzjrrrGb7r7vuus2ev/7660k+jOpdunRJkowcOTIXXHBBjjnmmOyzzz657777ct111+W22277VM4RAAAAAAAAgBU4jCfJCSeckMrKyowdOzYvvvhiVltttYwcOTKdOnXKrbfempEjR6Zfv37p06dPTj/99Oy0006Ldfy11147t912Ww4//PCcd9556dGjRy699NIMHz58OZ0RAAAAAAAAAP+uolQqlVp7CBauoaEhdXV1uW3gFulYuUL/HQMAADQz5IFJrT0CAAAAAK1oQUutr69PbW3tx65dYb9jHAAAAAAAAIAVgzAOAAAAAAAAQKEJ4wAAAAAAAAAUmjAOAAAAAAAAQKEJ4wAAAAAAAAAUmjAOAAAAAAAAQKEJ4wAAAAAAAAAUmjAOAAAAAAAAQKEJ4wAAAAAAAAAUmjAOAAAAAAAAQKEJ4wAAAAAAAAAUmjAOAAAAAAAAQKEJ4wAAAAAAAAAUmjAOAAAAAAAAQKEJ4wAAAAAAAAAUmjAOAAAAAAAAQKEJ4wAAAAAAAAAUmjAOAAAAAAAAQKEJ4wAAAAAAAAAUmjAOAAAAAAAAQKEJ4wAAAAAAAAAUmjAOAAAAAAAAQKEJ4wAAAAAAAAAUmjAOAAAAAAAAQKEJ4wAAAAAAAAAUmjAOAAAAAAAAQKEJ4wAAAAAAAAAUmjAOAAAAAAAAQKEJ4wAAAAAAAAAUWmVrD8CiGXznHamtrW3tMQAAAAAAAAA+d1wxDgAAAAAAAEChCeMAAAAAAAAAFJowDgAAAAAAAEChCeMAAAAAAAAAFJowDgAAAAAAAEChCeMAAAAAAAAAFJowDgAAAAAAAEChCeMAAAAAAAAAFJowDgAAAAAAAEChCeMAAAAAAAAAFJowDgAAAAAAAEChCeMAAAAAAAAAFJowDgAAAAAAAEChCeMAAAAAAAAAFJowDgAAAAAAAEChCeMAAAAAAAAAFJowDgAAAAAAAEChVbb2ACyai//7jrSv7tDaYwAABTD67B1bewQAAAAAgE+VK8YBAAAAAAAAKDRhHAAAAAAAAIBCE8YBAAAAAAAAKDRhHAAAAAAAAIBCE8YBAAAAAAAAKDRhHAAAAAAAAIBCE8YBAAAAAAAAKDRhHAAAAAAAAIBCE8YBAAAAAAAAKDRhHAAAAAAAAIBCE8YBAAAAAAAAKDRhHAAAAAAAAIBCE8YBAAAAAAAAKDRhHAAAAAAAAIBCE8YBAAAAAAAAKDRhHAAAAAAAAIBCE8YBAAAAAAAAKDRhHAAAAAAAAIBCE8YBAAAAAAAAKDRhHAAAAAAAAIBCE8YBAAAAAAAAKDRhHAAAAAAAAIBCE8YBAAAAAAAAKDRhHAAAAAAAAIBCE8YBAAAAAAAAKDRhHAAAAAAAAIBCE8YBAAAAAAAAKDRhHAAAAAAAAIBCE8YBAAAAAAAAKDRhHAAAAAAAAIBCE8YBAAAAAAAAKDRh/FNUUVGRm266qbXHAAAAAAAAAFihCOPLwUknnZRNN920xfaXXnop22233ac/EAAAAAAAAMAKrLK1B/i0zJs3L1VVVa06w6qrrtqq7w8AAAAAAACwIvrcXjE+dOjQjB49OqNHj05dXV1WWWWVnHDCCSmVSkmSXr165dRTT82ee+6Z2traHHDAAUmSyZMnZ8stt0z79u3Ts2fPHHLIIXnnnXfKx73qqqsyYMCAdO7cOauuump22223vPrqq+XXJ06cmIqKitx7770ZMGBAOnTokC222CIzZsxIklx++eU5+eSTM3369FRUVKSioiKXX355kkW7lXpjY2MaGhqaPQAAAAAAAABYcp/bMJ4kV1xxRSorK/PQQw/lvPPOyznnnJNLL720/PpZZ52VTTbZJI8++mhOOOGEPPPMM9l2222z00475c9//nN+9atfZfLkyRk9enR5n/fffz+nnnpqpk+fnptuuimzZs3KXnvt1eK9jzvuuJx99tl5+OGHU1lZmX322SdJsvPOO+fII4/MRhttlJdeeikvvfRSdt5550U+p3HjxqWurq786Nmz55J/QAAAAAAAAACkorTgEuvPmaFDh+bVV1/NY489loqKiiTJD3/4w9xyyy3529/+ll69eqVfv375zW9+U95nv/32S9u2bXPxxReXt02ePDlDhgzJO++8k5qamhbv8/DDD+fLX/5yZs+enU6dOmXixInZaqutcs8992TYsGFJkttvvz3bb7993n333dTU1OSkk07KTTfdlGnTpjU7VkVFRX7zm9/kW9/61kLPq7GxMY2NjeXnDQ0N6dmzZ84Y9cu0r+6wJB8VAEAzo8/esbVHAAAAAABYag0NDamrq0t9fX1qa2s/du3n+orxr3zlK+UoniQDBw7MU089lfnz5ydJBgwY0Gz99OnTc/nll6dTp07lx/Dhw9PU1JSZM2cmSaZOnZodd9wxa665Zjp37pwhQ4YkSZ5//vlmx+rbt2/536uttlqSNLvl+pKqrq5ObW1tswcAAAAAAAAAS66ytQdYnjp27Njs+Zw5c3LggQfmkEMOabF2zTXXzDvvvJPhw4dn+PDhufrqq9O1a9c8//zzGT58eObNm9dsfbt27cr/XhDnm5qalsNZAAAAAAAAALA0Ptdh/E9/+lOz53/84x+z/vrrp23bth+5/ktf+lL+9re/Zb311vvI1//yl7/kjTfeyGmnnVb+bu+HH354seeqqqoqX7UOAAAAAAAAQOv6XN9K/fnnn88RRxyRGTNm5Nprr83555+fQw89dKHrjz322Pz+97/P6NGjM23atDz11FO5+eabM3r06CQfXjVeVVWV888/P88++2xuueWWnHrqqYs9V69evTJz5sxMmzYtr7/+erPvDAcAAAAAAADg0/W5DuN77rln3n333Wy22WYZNWpUDj300BxwwAELXd+3b99MmjQpTz75ZLbccsv069cvY8eOzeqrr54k6dq1ay6//PJcf/316dOnT0477bScddZZiz3XTjvtlG233TZbbbVVunbtmmuvvXaJzxEAAAAAAACApVNRKpVKrT3Ekhg6dGg23XTTjB8/vrVHWa4aGhpSV1eXM0b9Mu2rO7T2OABAAYw+e8fWHgEAAAAAYKktaKn19fWpra392LWf6yvGAQAAAAAAAOCTCOMAAAAAAAAAFFplaw+wpCZOnNjaIwAAAAAAAADwOeCKcQAAAAAAAAAKTRgHAAAAAAAAoNCEcQAAAAAAAAAKTRgHAAAAAAAAoNCEcQAAAAAAAAAKTRgHAAAAAAAAoNCEcQAAAAAAAAAKTRgHAAAAAAAAoNCEcQAAAAAAAAAKTRgHAAAAAAAAoNCEcQAAAAAAAAAKTRgHAAAAAAAAoNCEcQAAAAAAAAAKbYnD+FVXXZVBgwZl9dVXz3PPPZckGT9+fG6++eZlNhwAAAAAAAAALK0lCuMXXXRRjjjiiPznf/5n3n777cyfPz9J0qVLl4wfP35ZzgcAAAAAAAAAS2WJwvj555+fn/70pznuuOPStm3b8vYBAwbkL3/5yzIbDgAAAAAAAACW1hKF8ZkzZ6Zfv34ttldXV+edd95Z6qEAAAAAAAAAYFlZojC+9tprZ9q0aS2233nnnendu/fSzgQAAAAAAAAAy0zlkux0xBFHZNSoUXnvvfdSKpXy0EMP5dprr824ceNy6aWXLusZAQAAAAAAAGCJLVEY32+//dK+ffscf/zxmTt3bnbbbbesvvrqOe+887LLLrss6xkBAAAAAAAAYIktdhj/4IMPcs0112T48OHZfffdM3fu3MyZMyfdunVbHvMBAAAAAAAAwFJZ7O8Yr6yszMiRI/Pee+8lSTp06CCKAwAAAAAAAPCZtdhhPEk222yzPProo8t6FgAAAAAAAABY5pboO8YPPvjgHHnkkfn73/+e/v37p2PHjs1e79u37zIZDgAAAAAAAACW1hKF8V122SVJcsghh5S3VVRUpFQqpaKiIvPnz1820wEAAAAAAADAUlqiMD5z5sxlPQef4MCfbJfa2trWHgMAAAAAAADgc2eJwvhaa621rOcAAAAAAAAAgOViicL4lVde+bGv77nnnks0DAAAAAAAAAAsaxWlUqm0uDuttNJKzZ6///77mTt3bqqqqtKhQ4e8+eaby2zAFV1DQ0Pq6upSX1/vVuoAAAAAAAAA/9/itNQ2S/IGb731VrPHnDlzMmPGjAwePDjXXnvtEg0NAAAAAAAAAMvDEoXxj7L++uvntNNOy6GHHrqsDgkAAAAAAAAAS22ZhfEkqayszIsvvrgsDwkAAAAAAAAAS6VySXa65ZZbmj0vlUp56aWXcsEFF2TQoEHLZDAAAAAAAAAAWBaWKIx/61vfava8oqIiXbt2zde+9rWcffbZy2IuAAAAAAAAAFgmliiMNzU1Les5AAAAAAAAAGC5WKLvGD/llFMyd+7cFtvffffdnHLKKUs9FAAAAAAAAAAsKxWlUqm0uDu1bds2L730Urp169Zs+xtvvJFu3bpl/vz5y2zAFV1DQ0Pq6upSX1+f2tra1h4HAAAAAAAA4DNhcVrqEl0xXiqVUlFR0WL79OnTs/LKKy/JIQEAAAAAAABguVis7xhfaaWVUlFRkYqKimywwQbN4vj8+fMzZ86cjBw5cpkPCQAAAAAAAABLarHC+Pjx41MqlbLPPvvk5JNPTl1dXfm1qqqq9OrVKwMHDlzmQwIAAAAAAADAklqsMD5ixIgkydprr50tttgi7dq1Wy5DAQAAAAAAAMCyslhhfIEhQ4aU//3ee+9l3rx5zV7/pC82BwAAAAAAAIBPS5sl2Wnu3LkZPXp0unXrlo4dO2allVZq9gAAAAAAAACAz4olCuNHH3107rvvvlx00UWprq7OpZdempNPPjmrr756rrzyymU9IwAAAAAAAAAssYpSqVRa3J3WXHPNXHnllRk6dGhqa2vzyCOPZL311stVV12Va6+9NrfffvvymHWF1NDQkLq6uhz//W+kxne6A7S6435xQ2uPAAAAAAAA5J8ttb6+/hO/7nuJrhh/8803s8466yT58PvE33zzzSTJ4MGD88ADDyzJIQEAAAAAAABguViiML7OOutk5syZSZINN9ww1113XZLk1ltvTZcuXZbZcAAAAAAAAACwtJYojO+9996ZPn16kuSHP/xh/vd//zc1NTU5/PDDc/TRRy/TAQEAAAAAAABgaVQuyU6HH354+d9bb711nnjiiUydOjXrrbde+vbtu8yGAwAAAAAAAICltURh/F+99957WWuttbLWWmsti3kAAAAAAAAAYJlaolupz58/P6eeemrWWGONdOrUKc8++2yS5IQTTsjPfvazZTogAAAAAAAAACyNJQrjP/7xj3P55ZfnjDPOSFVVVXn7xhtvnEsvvXSZDQcAAAAAAAAAS2uJwviVV16ZSy65JLvvvnvatm1b3r7JJpvkiSeeWGbDAQAAAAAAAMDSWqIw/o9//CPrrbdei+1NTU15//33l3ooAAAAAAAAAFhWliiM9+nTJw8++GCL7TfccEP69eu31EMBAAAAAAAAwLJSuSQ7jR07NiNGjMg//vGPNDU15de//nVmzJiRK6+8Mr/97W+X9YwAAAAAAAAAsMQW64rxZ599NqVSKd/85jdz66235p577knHjh0zduzYPP7447n11luzzTbbLK9ZAQAAAAAAAGCxLdYV4+uvv35eeumldOvWLVtuuWVWXnnl/OUvf0n37t2X13wAAAAAAAAAsFQW64rxUqnU7Pkdd9yRd955Z5kOBAAAAAAAAADL0mKF8X/376EcAAAAAAAAAD5rFiuMV1RUpKKiosU2AAAAAAAAAPisWqzvGC+VStlrr71SXV2dJHnvvfcycuTIdOzYsdm6X//618tuQgAAAAAAAABYCosVxkeMGNHs+R577LFMhwEAAAAAAACAZW2xwvjPf/7z5TUHAAAAAAAAACwXi/Ud4wAAAAAAAADweSOMAwAAAAAAAFBowjgAAAAAAAAAhSaMAwAAAAAAAFBowjgAAAAAAAAAhSaMAwAAAAAAAFBowjgAAAAAAAAAhSaMAwAAAAAAAFBowjgAAAAAAAAAhSaMAwAAAAAAAFBowjgAAAAAAAAAhSaML4VevXpl/PjxrT0GAAAAAAAAAB9DGF8El19+ebp06dLaYwAAAAAAAACwBIRxAAAAAAAAAApthQ3jv/3tb9OlS5fMnz8/STJt2rRUVFTkhz/8YXnNfvvtlx49emTvvfdOfX19KioqUlFRkZNOOukjj3nppZemS5cuuffee5Mks2fPzu67756OHTtmtdVWy7nnnpuhQ4fmsMMOW96nBwAAAAAAAMD/V9naA7SWLbfcMrNnz86jjz6aAQMGZNKkSVlllVUyceLE8ppJkyZl7NixeffddzN27NjMmDEjSdKpU6cWxzvjjDNyxhln5He/+10222yzJMkRRxyRKVOm5JZbbkn37t0zduzYPPLII9l0000XOldjY2MaGxvLzxsaGpbNCQMAAAAAAACsoFbYK8br6uqy6aablkP4xIkTc/jhh+fRRx/NnDlz8o9//CNPP/10ttpqq9TV1aWioiKrrrpqVl111RZh/Nhjj8348eMzadKkchSfPXt2rrjiipx11lkZNmxYNt544/z85z8vX6G+MOPGjUtdXV350bNnz+Vy/gAAAAAAAAArihU2jCfJkCFDMnHixJRKpTz44IP5zne+k969e2fy5MmZNGlSVl999ay//vofe4yzzz47P/3pTzN58uRstNFG5e3PPvts3n///XIoTz6M8V/4whc+9nhjxoxJfX19+fHCCy8s3UkCAAAAAAAArOBW6DA+dOjQTJ48OdOnT0+7du2y4YYbZujQoZk4cWImTZqUIUOGfOIxttxyy8yfPz/XXXfdMpmpuro6tbW1zR4AAAAAAAAALLkVOowv+J7xc889txzBF4TxiRMnZujQoUmSqqqqhd4CfbPNNssdd9yRn/zkJznrrLPK29dZZ520a9cu//d//1feVl9fnyeffHL5nRAAAAAAAAAALVS29gCtaaWVVkrfvn1z9dVX54ILLkiSfPWrX833v//9vP/+++VY3qtXr8yZMyf33ntvNtlkk3To0CEdOnQoH2eLLbbI7bffnu222y6VlZU57LDD0rlz54wYMSJHH310Vl555XTr1i0nnnhi2rRpk4qKilY5XwAAAAAAAIAV0Qp9xXjy4feMz58/v3x1+Morr5w+ffpk1VVXLX8f+BZbbJGRI0dm5513TteuXXPGGWe0OM7gwYNz22235fjjj8/555+fJDnnnHMycODA7LDDDtl6660zaNCg9O7dOzU1NZ/a+QEAAAAAAACs6CpKpVKptYdYUbzzzjtZY401cvbZZ2ffffddpH0aGhpSV1eX47//jdS0a7ecJwTgkxz3ixtaewQAAAAAACD/bKn19fWpra392LUr9K3Ul7dHH300TzzxRDbbbLPU19fnlFNOSZJ885vfbOXJAAAAAAAAAFYcwvhydtZZZ2XGjBmpqqpK//798+CDD2aVVVZp7bEAAAAAAAAAVhjC+HLUr1+/TJ06tbXHAAAAAAAAAFihtWntAQAAAAAAAABgeRLGAQAAAAAAACg0YRwAAAAAAACAQhPGAQAAAAAAACg0YRwAAAAAAACAQhPGAQAAAAAAACg0YRwAAAAAAACAQhPGAQAAAAAAACg0YRwAAAAAAACAQhPGAQAAAAAAACg0YRwAAAAAAACAQhPGAQAAAAAAACg0YRwAAAAAAACAQhPGAQAAAAAAACg0YRwAAAAAAACAQhPGAQAAAAAAACg0YRwAAAAAAACAQhPGAQAAAAAAACg0YRwAAAAAAACAQhPGAQAAAAAAACg0YRwAAAAAAACAQhPGAQAAAAAAACg0YRwAAAAAAACAQhPGAQAAAAAAACg0YRwAAAAAAACAQhPGAQAAAAAAACi0ilKpVGrtIVi4hoaG1NXVpb6+PrW1ta09DgAAAAAAAMBnwuK0VFeMAwAAAAAAAFBowjgAAAAAAAAAhSaMAwAAAAAAAFBowjgAAAAAAAAAhSaMAwAAAAAAAFBowjgAAAAAAAAAhSaMAwAAAAAAAFBowjgAAAAAAAAAhSaMAwAAAAAAAFBowjgAAAAAAAAAhSaMAwAAAAAAAFBowjgAAAAAAAAAhSaMAwAAAAAAAFBowjgAAAAAAAAAhSaMAwAAAAAAAFBowjgAAAAAAAAAhSaMAwAAAAAAAFBola09AItmxpmT0qmmY2uPAbDC633c11p7BAAAAAAAYDG5YhwAAAAAAACAQhPGAQAAAAAAACg0YRwAAAAAAACAQhPGAQAAAAAAACg0YRwAAAAAAACAQhPGAQAAAAAAACg0YRwAAAAAAACAQhPGAQAAAAAAACg0YRwAAAAAAACAQhPGAQAAAAAAACg0YRwAAAAAAACAQhPGAQAAAAAAACg0YRwAAAAAAACAQhPGAQAAAAAAACg0YRwAAAAAAACAQhPGAQAAAAAAACg0YRwAAAAAAACAQhPGAQAAAAAAACg0YRwAAAAAAACAQhPGAQAAAAAAACg0YRwAAAAAAACAQhPGAQAAAAAAACg0YRwAAAAAAACAQhPGAQAAAAAAACg0YRwAAAAAAACAQhPGAQAAAAAAACg0YRwAAAAAAACAQhPGAQAAAAAAACg0YRwAAAAAAACAQhPGAQAAAAAAACg0YRwAAAAAAACAQhPGAQAAAAAAACg0YRwAAAAAAACAQhPGF2LevHmtPQIAAAAAAAAAy4Aw/v8NHTo0o0ePzmGHHZZVVlklw4cPzznnnJMvfvGL6dixY3r27JmDDz44c+bMabbflClTMnTo0HTo0CErrbRShg8fnrfeeitJ0tTUlHHjxmXttddO+/bts8kmm+SGG2742DkaGxvT0NDQ7AEAAAAAAADAkhPG/8UVV1yRqqqqTJkyJRMmTEibNm3yP//zP3nsscdyxRVX5L777ssxxxxTXj9t2rQMGzYsffr0yR/+8IdMnjw5O+64Y+bPn58kGTduXK688spMmDAhjz32WA4//PDssccemTRp0kJnGDduXOrq6sqPnj17LvfzBgAAAAAAACiyilKpVGrtIT4Lhg4dmoaGhjzyyCMLXXPDDTdk5MiRef3115Mku+22W55//vlMnjy5xdrGxsasvPLKueeeezJw4MDy9v322y9z587NNddc85Hv0djYmMbGxvLzhoaG9OzZMw8df0s61XRc0tMDYBnpfdzXWnsEAAAAAAAgH7bUurq61NfXp7a29mPXVn5KM30u9O/fv9nze+65J+PGjcsTTzyRhoaGfPDBB3nvvfcyd+7cdOjQIdOmTcv3vve9jzzW008/nblz52abbbZptn3evHnp16/fQmeorq5OdXX10p8MAAAAAAAAAEmE8WY6dvznFdmzZs3KDjvskIMOOig//vGPs/LKK2fy5MnZd999M2/evHTo0CHt27df6LEWfBf5bbfdljXWWKPZa8I3AAAAAAAAwKdHGF+IqVOnpqmpKWeffXbatPnwq9ivu+66Zmv69u2be++9NyeffHKL/fv06ZPq6uo8//zzGTJkyKcyMwAAAAAAAAAtCeMLsd566+X999/P+eefnx133DFTpkzJhAkTmq0ZM2ZMvvjFL+bggw/OyJEjU1VVlfvvvz/f+973ssoqq+Soo47K4YcfnqampgwePDj19fWZMmVKamtrM2LEiFY6MwAAAAAAAIAVS5vWHuCzapNNNsk555yT008/PRtvvHGuvvrqjBs3rtmaDTbYIL/73e8yffr0bLbZZhk4cGBuvvnmVFZ++PcGp556ak444YSMGzcuvXv3zrbbbpvbbrsta6+9dmucEgAAAAAAAMAKqaJUKpVaewgWrqGhIXV1dXno+FvSqabjJ+8AwHLV+7ivtfYIAAAAAABA/tlS6+vrU1tb+7FrXTEOAAAAAAAAQKEJ4wAAAAAAAAAUmjAOAAAAAAAAQKEJ4wAAAAAAAAAUmjAOAAAAAAAAQKEJ4wAAAAAAAAAUmjAOAAAAAAAAQKEJ4wAAAAAAAAAUmjAOAAAAAAAAQKEJ4wAAAAAAAAAUmjAOAAAAAAAAQKEJ4wAAAAAAAAAUmjAOAAAAAAAAQKEJ4wAAAAAAAAAUmjAOAAAAAAAAQKEJ4wAAAAAAAAAUmjAOAAAAAAAAQKEJ4wAAAAAAAAAUmjAOAAAAAAAAQKEJ4wAAAAAAAAAUmjAOAAAAAAAAQKEJ4wAAAAAAAAAUmjAOAAAAAAAAQKEJ4wAAAAAAAAAUmjAOAAAAAAAAQKEJ4wAAAAAAAAAUmjAOAAAAAAAAQKEJ4wAAAAAAAAAUWmVrD8Ci+cLRQ1JbW9vaYwAAAAAAAAB87rhiHAAAAAAAAIBCE8YBAAAAAAAAKDRhHAAAAAAAAIBCE8YBAAAAAAAAKDRhHAAAAAAAAIBCE8YBAAAAAAAAKDRhHAD+X3t3Hmx1fd9//HXZLgheVFpZEhQkLsiORAeIogmKNtJgrERi41a3jlQRN0gUF1BcU2M0KiYjVmXcIUZFQ4kYwQ1kiVaKQnHUVnBp5Com6HDv7w+b+/vdn0psAh7yuY/HzJnJ+ZzvOd/392byHcdnPucAAAAAAABFE8YBAAAAAAAAKJowDgAAAAAAAEDRhHEAAAAAAAAAiiaMAwAAAAAAAFA0YRwAAAAAAACAognjAAAAAAAAABRNGAcAAAAAAACgaMI4AAAAAAAAAEUTxgEAAAAAAAAomjAOAAAAAAAAQNFaVHoAPp+pU6emurq60mPAn+zCCy+s9AgAAAAAAAA0UXaMAwAAAAAAAFA0YRwAAAAAAACAognjAAAAAAAAABRNGAcAAAAAAACgaMI4AAAAAAAAAEUTxgEAAAAAAAAomjAOAAAAAAAAQNGEcQAAAAAAAACKJowDAAAAAAAAUDRhHAAAAAAAAICiCeMAAAAAAAAAFE0YBwAAAAAAAKBowjgAAAAAAAAARRPGAQAAAAAAACiaMA4AAAAAAABA0YRxAAAAAAAAAIomjAMAAAAAAABQNGEcAAAAAAAAgKIJ4wAAAAAAAAAUTRgHAAAAAAAAoGjCOAAAAAAAAABFE8YBAAAAAAAAKJowDgAAAAAAAEDRhHEAAAAAAAAAiiaMAwAAAAAAAFA0YRwAAAAAAACAognjAAAAAAAAABRNGAcAAAAAAACgaMI4AAAAAAAAAEUTxgEAAAAAAAAomjAOAAAAAAAAQNGEcQAAAAAAAACKJoz/iVasWJEDDjggHTt2TOvWrbPLLrvkvPPOy0cffdTouHvuuSd77LFHWrdunT59+uThhx+u0MQAAAAAAAAATVOLSg/wl6ply5Y5+uijM3DgwGy33XZZtmxZTjzxxNTV1eXSSy9Nkjz55JMZM2ZMpk6dmkMPPTQzZszIqFGjsnjx4vTu3bvCVwAAAAAAAADQNAjjm/DKK6+ke/fun1gfNmxY5s2bl1122aVhbeedd868efPyxBNPNKz96Ec/ysEHH5yzzz47STJ58uTMmTMn1113XW688cYtfwEAAAAAAAAA+Cr1TenatWveeOONhseSJUvSoUOH7Lfffp84duXKlXnkkUcybNiwhrWnnnoqw4cPb3TciBEj8tRTT33mOTds2JDa2tpGDwAAAAAAAAD+dML4JjRv3jydOnVKp06dst122+WUU07J4MGDc+GFFzYcM2TIkLRu3Tq77rpr9t1331x88cUNr61ZsyYdO3Zs9JkdO3bMmjVrPvOcU6dOTfv27RseXbt23ezXBQAAAAAAANCUCOOf0/HHH5/33nsvM2bMSLNm//fPdtddd2Xx4sWZMWNGHnrooVx11VV/1nkmTpyYdevWNTxee+21P3d0AAAAAAAAgCbNb4x/DlOmTMmjjz6aZ599Nttuu22j1/6wo3vPPffMxo0bc9JJJ+XMM89s2G2+du3aRsevXbs2nTp1+sxzVVdXp7q6evNfBAAAAAAAAEATZcf4H3Hffffl4osvzt13350ePXps8ti6urp89NFHqaurS5IMHjw4c+fObXTMnDlzMnjw4C02LwAAAAAAAACN2TG+CS+88EKOPvronHvuuenVq1fDb4O3atUqs2fPTsuWLdOnT59UV1dn0aJFmThxYr7zne+kZcuWSZLTTz89w4YNy9VXX51vfvObufPOO7No0aJMmzatkpcFAAAAAAAA0KTYMb4JixYtygcffJApU6akc+fODY9vf/vbadGiRS6//PLsvffe6du3by666KKMHTs2P/3pTxveP2TIkMyYMSPTpk1Lv379cu+992bWrFnp3bt3Ba8KAAAAAAAAoGmpqq+vr6/0EHy22tratG/fPhMmTPDb4/xFu/DCCys9AgAAAAAAAAX5Q0tdt25dampqNnmsHeMAAAAAAAAAFE0YBwAAAAAAAKBowjgAAAAAAAAARRPGAQAAAAAAACiaMA4AAAAAAABA0YRxAAAAAAAAAIomjAMAAAAAAABQNGEcAAAAAAAAgKIJ4wAAAAAAAAAUTRgHAAAAAAAAoGjCOAAAAAAAAABFE8YBAAAAAAAAKJowDgAAAAAAAEDRhHEAAAAAAAAAiiaMAwAAAAAAAFA0YRwAAAAAAACAognjAAAAAAAAABRNGAcAAAAAAACgaMI4AAAAAAAAAEUTxgEAAAAAAAAomjAOAAAAAAAAQNGEcQAAAAAAAACKJowDAAAAAAAAUDRhHAAAAAAAAICiCeMAAAAAAAAAFE0YBwAAAAAAAKBowjgAAAAAAAAARRPGAQAAAAAAACiaMA4AAAAAAABA0arq6+vrKz0En622tjbt27fPunXrUlNTU+lxAAAAAAAAALYK/5uWasc4AAAAAAAAAEUTxgEAAAAAAAAomjAOAAAAAAAAQNGEcQAAAAAAAACKJowDAAAAAAAAUDRhHAAAAAAAAICiCeMAAAAAAAAAFE0YBwAAAAAAAKBowjgAAAAAAAAARRPGAQAAAAAAACiaMA4AAAAAAABA0YRxAAAAAAAAAIomjAMAAAAAAABQNGEcAAAAAAAAgKIJ4wAAAAAAAAAUTRgHAAAAAAAAoGjCOAAAAAAAAABFa1HpAfh87p95QLbZpnmlx4A/2egjnq30CAAAAAAAADRRdowDAAAAAAAAUDRhHAAAAAAAAICiCeMAAAAAAAAAFE0YBwAAAAAAAKBowjgAAAAAAAAARRPGAQAAAAAAACiaMA4AAAAAAABA0YRxAAAAAAAAAIomjAMAAAAAAABQNGEcAAAAAAAAgKIJ4wAAAAAAAAAUTRgHAAAAAAAAoGjCOAAAAAAAAABFE8YBAAAAAAAAKJowDgAAAAAAAEDRhHEAAAAAAAAAiiaMAwAAAAAAAFA0YRwAAAAAAACAognjAAAAAAAAABRNGAcAAAAAAACgaMI4AAAAAAAAAEUTxgEAAAAAAAAomjAOAAAAAAAAQNGEcQAAAAAAAACKJowDAAAAAAAAUDRhHAAAAAAAAICiCeMAAAAAAAAAFE0YBwAAAAAAAKBowjgAAAAAAAAARRPGAQAAAAAAACiaMA4AAAAAAABA0YRxAAAAAAAAAIomjAMAAAAAAABQNGEcAAAAAAAAgKIJ45vwyCOP5Gtf+1q22267dOjQIYceemhWrVrV8PqTTz6Z/v37p3Xr1hk0aFBmzZqVqqqqLF26tOGYF154IYccckjatWuXjh075nvf+17efvvtzzznhg0bUltb2+gBAAAAAAAAwJ9OGN+E9evXZ/z48Vm0aFHmzp2bZs2a5bDDDktdXV1qa2szcuTI9OnTJ4sXL87kyZNz7rnnNnr/u+++m69//esZMGBAFi1alEceeSRr167N6NGjP/OcU6dOTfv27RseXbt23dKXCQAAAAAAAFC0qvr6+vpKD/GX4u23385f//Vf5/nnn8/8+fNz3nnn5fXXX0/r1q2TJD/96U9z4oknZsmSJenfv3+mTJmSJ554Io8++mjDZ7z++uvp2rVrVqxYkd122+0T59iwYUM2bNjQ8Ly2tjZdu3bNLdMHZpttmm/5i4QtZPQRz1Z6BAAAAAAAAApSW1ub9u3bZ926dampqdnksS2+oJn+Ir388suZNGlSnnnmmbz99tupq6tLkrz66qtZsWJF+vbt2xDFk2Tvvfdu9P5ly5blscceS7t27T7x2atWrfrUMF5dXZ3q6urNfCUAAAAAAAAATZcwvgkjR47MzjvvnJtvvjldunRJXV1devfunQ8//PBzvf/999/PyJEjc/nll3/itc6dO2/ucQEAAAAAAAD4FML4Z3jnnXeyYsWK3Hzzzdl3332TJPPnz294fffdd8/tt9+eDRs2NOzwXrhwYaPPGDhwYO67775069YtLVr4UwMAAAAAAABUQrNKD7C12n777dOhQ4dMmzYtK1euzK9+9auMHz++4fXvfve7qaury0knnZTly5fn0UcfzVVXXZUkqaqqSpKceuqp+e///u+MGTMmCxcuzKpVq/Loo4/muOOOy8aNGytyXQAAAAAAAABNjTD+GZo1a5Y777wzzz33XHr37p0zzjgjV155ZcPrNTU1+cUvfpGlS5emf//++cEPfpBJkyYlScPvjnfp0iULFizIxo0bc9BBB6VPnz4ZN25ctttuuzRr5k8PAAAAAAAA8EXw/d6bMHz48Lz44ouN1urr6xv+85AhQ7Js2bKG53fccUdatmyZnXbaqWFt1113zf3337/lhwUAAAAAAADgUwnjf4Z/+Zd/yS677JIvfelLWbZsWc4999yMHj06bdq0qfRoAAAAAAAAAPwPYfzPsGbNmkyaNClr1qxJ586dc8QRR+SSSy6p9FgAAAAAAAAA/D+E8T/DOeeck3POOafSYwAAAAAAAACwCc0qPQAAAAAAAAAAbEnCOAAAAAAAAABFE8YBAAAAAAAAKJowDgAAAAAAAEDRhHEAAAAAAAAAiiaMAwAAAAAAAFA0YRwAAAAAAACAognjAAAAAAAAABRNGAcAAAAAAACgaMI4AAAAAAAAAEUTxgEAAAAAAAAomjAOAAAAAAAAQNGEcQAAAAAAAACKJowDAAAAAAAAUDRhHAAAAAAAAICiCeMAAAAAAAAAFE0YBwAAAAAAAKBowjgAAAAAAAAARRPGAQAAAAAAACiaMA4AAAAAAABA0YRxAAAAAAAAAIomjAMAAAAAAABQNGEcAAAAAAAAgKIJ4wAAAAAAAAAUTRgHAAAAAAAAoGgtKj0An8+3D3ssNTU1lR4DAAAAAAAA4C+OHeMAAAAAAAAAFE0YBwAAAAAAAKBowjgAAAAAAAAARRPGAQAAAAAAACiaMA4AAAAAAABA0YRxAAAAAAAAAIomjAMAAAAAAABQNGEcAAAAAAAAgKIJ4wAAAAAAAAAUTRgHAAAAAAAAoGjCOAAAAAAAAABFE8YBAAAAAAAAKJowDgAAAAAAAEDRhHEAAAAAAAAAiiaMAwAAAAAAAFA0YRwAAAAAAACAognjAAAAAAAAABRNGAcAAAAAAACgaMI4AAAAAAAAAEUTxgEAAAAAAAAomjAOAAAAAAAAQNGEcQAAAAAAAACKJowDAAAAAAAAUDRhHAAAAAAAAICiCeMAAAAAAAAAFE0YBwAAAAAAAKBowjgAAAAAAAAARRPGAQAAAAAAACiaMA4AAAAAAABA0YRxAAAAAAAAAIomjAMAAAAAAABQNGEcAAAAAAAAgKIJ4wAAAAAAAAAUTRgHAAAAAAAAoGjCOAAAAAAAAABFE8YBAAAAAAAAKJowDgAAAAAAAEDRhHEAAAAAAAAAiiaMAwAAAAAAAFA0YRwAAAAAAACAognjAAAAAAAAABRNGAcAAAAAAACgaMI4AAAAAAAAAEUTxgEAAAAAAAAomjAOAAAAAAAAQNGEcQAAAAAAAACKJowDAAAAAAAAUDRhHAAAAAAAAICiCeMAAAAAAAAAFE0YBwAAAAAAAKBowvj/p6qqKrNmzar0GAAAAAAAAABsJsI4AAAAAAAAAEWraBi/995706dPn7Rp0yYdOnTI8OHD8/jjj6dly5ZZs2ZNo2PHjRuXfffdN0kyffr0bLfddnnwwQez++67Z5tttsnf/d3f5YMPPsitt96abt26Zfvtt89pp52WjRs3NnxGt27dMnny5IwZMyZt27bNl770pVx//fWNXk+Sww47LFVVVQ3Pk+SGG25Ijx490qpVq+y+++657bbbGs1XVVWVm266KYceemi22Wab9OzZM0899VRWrlyZ/fffP23bts2QIUOyatWqzfxXBAAAAAAAAGBTKhbG33jjjYwZMybHH398li9fnnnz5uXb3/529tprr+yyyy6NwvNHH32UO+64I8cff3zD2gcffJBrr702d955Zx555JHMmzcvhx12WB5++OE8/PDDue2223LTTTfl3nvvbXTeK6+8Mv369cuSJUsyYcKEnH766ZkzZ06SZOHChUmSW265JW+88UbD85kzZ+b000/PmWeemRdeeCEnn3xyjjvuuDz22GONPnvy5Mk5+uijs3Tp0uyxxx757ne/m5NPPjkTJ07MokWLUl9fn7Fjx27y77Jhw4bU1tY2egAAAAAAAADwp6uqr6+vr8SJFy9enL322iuvvPJKdt5550avXXHFFZk+fXpefPHFJMn999+fY445JmvWrEnbtm0zffr0HHfccVm5cmV69OiRJDnllFNy2223Ze3atWnXrl2S5OCDD063bt1y4403Jvl4R3jPnj0ze/bshnMdeeSRqa2tzcMPP5zk453fM2fOzKhRoxqOGTp0aHr16pVp06Y1rI0ePTrr16/PQw891PC+8847L5MnT06SPP300xk8eHB+9rOfNQT9O++8M8cdd1x+97vffebf5cILL8xFF130ifV169alpqbmc/xlAQAAAAAAAMpXW1ub9u3bf66WWrEd4/369cs3vvGN9OnTJ0cccURuvvnm/Pa3v02SHHvssVm5cmWefvrpJB9/dfro0aPTtm3bhvdvs802DVE8STp27Jhu3bo1RPE/rL355puNzjt48OBPPF++fPkmZ12+fHmGDh3aaG3o0KGfeF/fvn0bnTtJ+vTp02jt97///SZ3gU+cODHr1q1reLz22mubnA0AAAAAAACATatYGG/evHnmzJmT2bNnZ88998yPf/zj7L777lm9enV23HHHjBw5MrfcckvWrl2b2bNnN/oa9SRp2bJlo+dVVVWfulZXV7fFr+XTZqqqqvrMtU3NVF1dnZqamkYPAAAAAAAAAP50FQvjyceheOjQobnooouyZMmStGrVKjNnzkySnHDCCbnrrrsybdq09OjR4xM7tv9Uf9iF/v8+79mzZ8Pzli1bZuPGjY2O6dmzZxYsWNBobcGCBdlzzz03y0wAAAAAAAAAbDktKnXiZ555JnPnzs1BBx2UHXfcMc8880zeeuuthkg9YsSI1NTUZMqUKbn44os323kXLFiQK664IqNGjcqcOXNyzz33NPxOePLx75DPnTs3Q4cOTXV1dbbffvucffbZGT16dAYMGJDhw4fnF7/4Re6///7867/+62abCwAAAAAAAIAto2I7xmtqavLrX/86f/M3f5Pddtst5513Xq6++uoccsghHw/WrFmOPfbYbNy4MUcfffRmO++ZZ56ZRYsWZcCAAZkyZUp++MMfZsSIEQ2vX3311ZkzZ066du2aAQMGJElGjRqVH/3oR7nqqqvSq1ev3HTTTbnllluy//77b7a5AAAAAAAAANgyqurr6+srPcRn+Yd/+Ie89dZbeeCBBzbL53Xr1i3jxo3LuHHjNsvnfRFqa2vTvn37rFu3zu+NAwAAAAAAAPyP/01LrdhXqW/KunXr8vzzz2fGjBmbLYoDAAAAAAAA0DRtlWH8W9/6Vp599tmccsopOfDAAys9DgAAAAAAAAB/wbbKMD5v3rwt8rmvvPLKFvlcAAAAAAAAALZezSo9AAAAAAAAAABsScI4AAAAAAAAAEUTxgEAAAAAAAAomjAOAAAAAAAAQNGEcQAAAAAAAACKJowDAAAAAAAAUDRhHAAAAAAAAICiCeMAAAAAAAAAFE0YBwAAAAAAAKBowjgAAAAAAAAARRPGAQAAAAAAACiaMA4AAAAAAABA0YRxAAAAAAAAAIomjAMAAAAAAABQNGEcAAAAAAAAgKIJ4wAAAAAAAAAUTRgHAAAAAAAAoGjCOAAAAAAAAABFE8YBAAAAAAAAKJowDgAAAAAAAEDRhHEAAAAAAAAAiiaMAwAAAAAAAFA0YRwAAAAAAACAognjAAAAAAAAABRNGAcAAAAAAACgaMI4AAAAAAAAAEUTxgEAAAAAAAAomjAOAAAAAAAAQNGEcQAAAAAAAACKJowDAAAAAAAAUDRhHAAAAAAAAICiCeMAAAAAAAAAFE0YBwAAAAAAAKBowjgAAAAAAAAARWtR6QHYtPr6+iRJbW1thScBAAAAAAAA2Hr8oaH+oaluijC+lXvnnXeSJF27dq3wJAAAAAAAAABbn/feey/t27ff5DHC+FZuhx12SJK8+uqrf/S/TAC2rNra2nTt2jWvvfZaampqKj0OQJPmngyw9XBPBth6uCcDbF3cl7e8+vr6vPfee+nSpcsfPVYY38o1a/bxz8C3b9/e/2AAthI1NTXuyQBbCfdkgK2HezLA1sM9GWDr4r68ZX3ezcXNtvAcAAAAAAAAAFBRwjgAAAAAAAAARRPGt3LV1dW54IILUl1dXelRAJo892SArYd7MsDWwz0ZYOvhngywdXFf3rpU1dfX11d6CAAAAAAAAADYUuwYBwAAAAAAAKBowjgAAAAAAAAARRPGAQAAAAAAACiaMA4AAAAAAABA0YTxrdj111+fbt26pXXr1tlnn33y7LPPVnokgCZn6tSp+epXv5ptt902O+64Y0aNGpUVK1ZUeiwAklx22WWpqqrKuHHjKj0KQJP1n//5n/n7v//7dOjQIW3atEmfPn2yaNGiSo8F0ORs3Lgx559/frp37542bdqkR48emTx5curr6ys9GkDxfv3rX2fkyJHp0qVLqqqqMmvWrEav19fXZ9KkSencuXPatGmT4cOH5+WXX67MsE2cML6VuuuuuzJ+/PhccMEFWbx4cfr165cRI0bkzTffrPRoAE3K448/nlNPPTVPP/105syZk48++igHHXRQ1q9fX+nRAJq0hQsX5qabbkrfvn0rPQpAk/Xb3/42Q4cOTcuWLTN79uy8+OKLufrqq7P99ttXejSAJufyyy/PDTfckOuuuy7Lly/P5ZdfniuuuCI//vGPKz0aQPHWr1+ffv365frrr//U16+44opce+21ufHGG/PMM8+kbdu2GTFiRH7/+99/wZNSVe//MrZV2mefffLVr3411113XZKkrq4uXbt2zT/90z9lwoQJFZ4OoOl66623suOOO+bxxx/PfvvtV+lxAJqk999/PwMHDsxPfvKTTJkyJf37988111xT6bEAmpwJEyZkwYIFeeKJJyo9CkCTd+ihh6Zjx4752c9+1rB2+OGHp02bNrn99tsrOBlA01JVVZWZM2dm1KhRST7eLd6lS5eceeaZOeuss5Ik69atS8eOHTN9+vQceeSRFZy26bFjfCv04Ycf5rnnnsvw4cMb1po1a5bhw4fnqaeequBkAKxbty5JssMOO1R4EoCm69RTT803v/nNRv+8DMAX74EHHsigQYNyxBFHZMcdd8yAAQNy8803V3osgCZpyJAhmTt3bl566aUkybJlyzJ//vwccsghFZ4MoGlbvXp11qxZ0+jfYbRv3z777LOP5lcBLSo9AJ/09ttvZ+PGjenYsWOj9Y4dO+bf//3fKzQVAHV1dRk3blyGDh2a3r17V3ocgCbpzjvvzOLFi7Nw4cJKjwLQ5P3Hf/xHbrjhhowfPz7f//73s3Dhwpx22mlp1apVjjnmmEqPB9CkTJgwIbW1tdljjz3SvHnzbNy4MZdcckmOOuqoSo8G0KStWbMmST61+f3hNb44wjgAfE6nnnpqXnjhhcyfP7/SowA0Sa+99lpOP/30zJkzJ61bt670OABNXl1dXQYNGpRLL700STJgwIC88MILufHGG4VxgC/Y3XffnTvuuCMzZsxIr169snTp0owbNy5dunRxTwaA/+Gr1LdCf/VXf5XmzZtn7dq1jdbXrl2bTp06VWgqgKZt7NixefDBB/PYY4/ly1/+cqXHAWiSnnvuubz55psZOHBgWrRokRYtWuTxxx/PtddemxYtWmTjxo2VHhGgSencuXP23HPPRms9e/bMq6++WqGJAJqus88+OxMmTMiRRx6ZPn365Hvf+17OOOOMTJ06tdKjATRpf+h6mt/WQRjfCrVq1Sp77bVX5s6d27BWV1eXuXPnZvDgwRWcDKDpqa+vz9ixYzNz5sz86le/Svfu3Ss9EkCT9Y1vfCPPP/98li5d2vAYNGhQjjrqqCxdujTNmzev9IgATcrQoUOzYsWKRmsvvfRSdt555wpNBNB0ffDBB2nWrPG/7m/evHnq6uoqNBEASdK9e/d06tSpUfOrra3NM888o/lVgK9S30qNHz8+xxxzTAYNGpS9994711xzTdavX5/jjjuu0qMBNCmnnnpqZsyYkZ///OfZdtttG373pX379mnTpk2FpwNoWrbddtv07t270Vrbtm3ToUOHT6wDsOWdccYZGTJkSC699NKMHj06zz77bKZNm5Zp06ZVejSAJmfkyJG55JJLstNOO6VXr15ZsmRJfvjDH+b444+v9GgAxXv//fezcuXKhuerV6/O0qVLs8MOO2SnnXbKuHHjMmXKlOy6667p3r17zj///HTp0iWjRo2q3NBNVFV9fX19pYfg01133XW58sors2bNmvTv3z/XXntt9tlnn0qPBdCkVFVVfer6LbfckmOPPfaLHQaAT9h///3Tv3//XHPNNZUeBaBJevDBBzNx4sS8/PLL6d69e8aPH58TTzyx0mMBNDnvvfdezj///MycOTNvvvlmunTpkjFjxmTSpElp1apVpccDKNq8efNywAEHfGL9mGOOyfTp01NfX58LLrgg06ZNy7vvvpuvfe1r+clPfpLddtutAtM2bcI4AAAAAAAAAEXzG+MAAAAAAAAAFE0YBwAAAAAAAKBowjgAAAAAAAAARRPGAQAAAAAAACiaMA4AAAAAAABA0YRxAAAAAAAAAIomjAMAAAAAAABQNGEcAAAAAAAAgKIJ4wAAAAAAAAAUTRgHAACAJuKtt97KP/7jP2annXZKdXV1OnXqlBEjRmTBggWVHg0AAAC2qBaVHgAAAAD4Yhx++OH58MMPc+utt2aXXXbJ2rVrM3fu3Lzzzjtb5HwffvhhWrVqtUU+GwAAAP437BgHAACAJuDdd9/NE088kcsvvzwHHHBAdt555+y9996ZOHFi/vZv/7bhmJNPPjkdO3ZM69at07t37zz44IMNn3HfffelV69eqa6uTrdu3XL11Vc3Oke3bt0yefLkHH300ampqclJJ52UJJk/f3723XfftGnTJl27ds1pp52W9evXf3EXDwAAQJMnjAMAAEAT0K5du7Rr1y6zZs3Khg0bPvF6XV1dDjnkkCxYsCC33357XnzxxVx22WVp3rx5kuS5557L6NGjc+SRR+b555/PhRdemPPPPz/Tp09v9DlXXXVV+vXrlyVLluT888/PqlWrcvDBB+fwww/Pb37zm9x1112ZP39+xo4d+0VcNgAAACRJqurr6+srPQQAAACw5d1333058cQT87vf/S4DBw7MsGHDcuSRR6Zv37755S9/mUMOOSTLly/Pbrvt9on3HnXUUXnrrbfyy1/+smHtnHPOyUMPPZR/+7d/S/LxjvEBAwZk5syZDceccMIJad68eW666aaGtfnz52fYsGFZv359WrduvQWvGAAAAD5mxzgAAAA0EYcffnj+67/+Kw888EAOPvjgzJs3LwMHDsz06dOzdOnSfPnLX/7UKJ4ky5cvz9ChQxutDR06NC+//HI2btzYsDZo0KBGxyxbtizTp09v2LHerl27jBgxInV1dVm9evXmv0gAAAD4FC0qPQAAAADwxWndunUOPPDAHHjggTn//PNzwgkn5IILLshZZ521WT6/bdu2jZ6///77Ofnkk3Paaad94tiddtpps5wTAAAA/hhhHAAAAJqwPffcM7NmzUrfvn3z+uuv56WXXvrUXeM9e/bMggULGq0tWLAgu+22W8PvkH+agQMH5sUXX8xXvvKVzT47AAAAfF6+Sh0AAACagHfeeSdf//rXc/vtt+c3v/lNVq9enXvuuSdXXHFFvvWtb2XYsGHZb7/9cvjhh2fOnDlZvXp1Zs+enUceeSRJcuaZZ2bu3LmZPHlyXnrppdx666257rrr/uhO83PPPTdPPvlkxo4dm6VLl+bll1/Oz3/+84wdO/aLuGwAAABIYsc4AAAANAnt2rXLPvvsk3/+53/OqlWr8tFHH6Vr16458cQT8/3vfz9Jct999+Wss87KmDFjsn79+nzlK1/JZZddluTjnd933313Jk2alMmTJ6dz5865+OKLc+yxx27yvH379s3jjz+eH/zgB9l3331TX1+fHj165Dvf+c6WvmQAAABoUFVfX19f6SEAAAAAAAAAYEvxVeoAAAAAAAAAFE0YBwAAAAAAAKBowjgAAAAAAAAARRPGAQAAAAAAACiaMA4AAAAAAABA0YRxAAAAAAAAAIomjAMAAAAAAABQNGEcAAAAAAAAgKIJ4wAAAAAAAAAUTRgHAAAAAAAAoGjCOAAAAAAAAABF+z8kGh6yMLwIqwAAAABJRU5ErkJggg==","text/plain":["<Figure size 2000x1000 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# Importance Score Top 10\n","feature_map_10 = feature_map.iloc[:10]\n","plt.figure(figsize=(20, 10))\n","sns.barplot(x=\"Score\", y=\"Feature\", data=feature_map_10.sort_values(by=\"Score\", ascending=False), errwidth=40)\n","plt.title('XGBoost Importance Features')\n","plt.tight_layout()\n","plt.show()"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOYjPhV7X11SOAtaishDu7i","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
