---
layout: post
title: Poetry란 무엇인가   
tags: [SW]
categories: [Development]
sitemap:
  changefreq: daily
  priority : 1.0
---

Poetry란 무엇인지 알아보고 사용하는 방법에 대해서 설명하고자 합니다.

----

목차
- toc
{: toc }

----  

### Poetry란?  

Poetry란 pip와 같이 패키지의 설치 및 관리를 해주는 의존성 관리자이다. pip를 사용해서 패키지를 설치하고 관리할 수 있지만 requirements.txt에서 패키지를 일일이 관리를 해줘야 한다. 게다가 requirements.txt는 자동으로 생성되지도 않아서 열심히 개발하고 있는 도중에 작성할려고 하면 어떤 패키지를 설치했는지 관리하기가 힘들어진다.  
또한, Poetry는 conda에서 python 가상환경을 만들어 주는 역할도 동시에 해주는 고마운 녀석이다. 실제로는 가상환경을 먼저 생성하고 생성된 가상환경에 들어가서 의존성 패키지를 설치하고 관리를 해야 하는데 poetry는 이를 동시에 작업을 할 수 있게 해준다.  
pip 생태계가 많이 퍼져 있어서 커뮤니티도 많이 활성화되어 있지만 conda 가상환경의 결합(아직 난 경험해보지 않아서 잘 모르지만)으로 인해 고생할 수도 있으니 미리 배워둘려고 한다.  

### Poetry install  



![그림](/assets/img/my_photo/Post_20240303_3.png)  

FS32(float32)을 기준으로 한다면 $$32 bits / 8 bits = 4 bytes$$ (1 byte는 8 bits 이므로)로 산출됩니다. 여기서, B(빌리온, $$10^9$$)과 GB(기가바이트, $$10^9$$)의 자릿수는 동일합니다.   

예를 들어 LLM 모델의 파라미터가 2B라고 가정한다면 FS32인 경우 아래와 같이 필요한 메모리를 추정할 수 있습니다.  
- LLM Load memory : 2B * 4 bytes = 8GB  
(모델 파라미터를 N개라고 할 경우 4N)  
- Optimizer(AdamW) : (2B * 4 bytes) * 4 = 32GB(16N)  
- Gradients : 2B * 4 bytes = 8GB(4N)  
- Total memory : 8GB + 32GB + 8GB = 48GB(24N)  

만약 inference만 필요한 경우에는 모델 메모리의 약 1.2배 정도 필요합니다. 자세한 내용은 참조 블로그를 읽어보시면 되겠습니다.  

[참조 블로그](https://blog.eleuther.ai/transformer-math/#total-inference-memory)  

### Test loss에 따른 최적값(계산투입량, 학습 데이터, 파라미터)  

원하는 Test loss가 있다면 아래 수식에 의해서 학습 데이터 및 파라미터의 크기를 추정할 수 있습니다. 반대로 학습 데이터와 모델의 파라미터가 정해졌다면 도달할 수 있는 loss 값을 사전에 미리 파악할 수 있습니다.  

Test Loss vs. Compute  
$$L = \left( \frac{C_{min}}{2.3 \cdot 10^8} \right)^{-0.050}$$    

Test Loss vs. Dataset Size   
$$L = \left( \frac{D}{5.4 \cdot 10^{13}} \right)^{-0.095}$$  

Test Loss vs. Parameters  
$$L = \left( \frac{N}{8.8 \cdot 10^3} \right)^{-0.076}$$  

![그림](/assets/img/my_photo/Post_20240303_1.png)  
[Kaplan et al, “Scaling Laws for Neural Language Models”, OpenAI, 2020](https://arxiv.org/pdf/2001.08361.pdf)  

아래 그림은 GPT3 모델를 위해서 최적의 파라미터를 추정하기 위한 실험 결과입니다.   
![그림](/assets/img/my_photo/Post_20240303_2.png)    
[Brown et al, “Language Models are Few-Shot Learners”, NeurIPS, 2020](https://arxiv.org/pdf/2005.14165.pdf)
