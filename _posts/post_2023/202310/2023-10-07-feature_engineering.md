---
layout: post
title: Feature Engineering(피처 엔지니어링)   
tags: [ML]
categories: [ML]
sitemap:
  changefreq: daily
  priority : 1.0
---

피처 엔지니어링(Feature Engineering)
+ 피처 엔지니어링은 목적에 맞게 가공하는 과정이다. 
+ 분석가는 데이터에서 분석에 필요한 피처를 추출하여 모델링에 사용한다.
+ 어떤 피처를 추출하느냐는 모델링을 통한 피드백과 분석가의 직관에 의해 이루어진다.
+ 피처를 잘못 선택했을 때 발생하는 문제점들에 대해 인지한다.

----

목차
- toc
{: toc }

----  

### 📝메모 

피처 엔지니어링(Feature Engineering)
+ 피처 엔지니어링은 목적에 맞게 가공하는 과정이다. 
+ 분석가는 데이터에서 분석에 필요한 피처를 추출하여 모델링에 사용한다.
+ 어떤 피처를 추출하느냐는 모델링을 통한 피드백과 분석가의 직관에 의해 이루어진다.
+ 피처를 잘못 선택했을 때 발생하는 문제점들에 대해 인지한다.

모델링을 통한 피드백(evaluation metric) -> 코드 또는 모델 무작위 수행 관점
분석가의 직관(domain knowledge = data literacy) -> 도메인 지식에 의존

**차원의 저주 - 고차원 데이터에 취약한 머신러닝 문제** -> 딥러닝으로 해결 가능하나 머신러닝으로 해결해야 한다면 고차원을에 대해서 취약하다는 뜻(현재 약 100차원 이상을 고차원이라고 봄)

+ feature space의 차원이 커질수록 머신러닝 모델의 예측 성능이 저하되는 문제
+ 유의미한 패턴을 찾기가 힘들어서 overfitting될 가능성이 높아짐
+ 거리값들의 차이가 무의미해짐
+ 차원 감소 기법(dimensionality reduction)을 이용하여 저차원 공간으로 변환
	+ feature extraction
	+ feature selection
+ PCA(Principal Component Analysis), AutoEncoder(non-linear PCA) 등을 많이 사용

space grow exponentially 예시 -> 차원이 늘어날 수록 점차 표현할 수 있는 공간이 줄어든다. 2차원에서 (1,0)은 $x_1$이 1이지만 $x_2$가 0이므로 $x_2$가 없기 때문에 결국 1차원 데이터이다. 따라서 2차원에서는 원의 방정식과 유사하게 $x_{1}^2 + x_{2}^2 = y^2$의 영역에서만 2개의 데이터로 표현할 수 있다는 뜻이다.

![[Pasted image 20231005161915.png]]


상관관계 vs. 인과관계
상관관계(correlation)은 인과관계(casuality)를 나타내지는 않는다. 

Feature Scaling
+ 서로 다른 feature들이 갑을 가지는 범위가 달라서 모델링에서 문제가 발생할 수 있음
+ Feature끼리 비교하려면 동일한 범위 내에 존재해야 함
+ Scaling을 통해서 피처들의 크기를 맞춤
+ 피처의 스케일과 무관한 모델들도 있음(e.g. LightGBM)
+ Min-Max Scaling(0~1)
$$x' = {{x - x_{min}} \over {x_{max} - x_{min}}}$$
+ Standard Scaling($-\infty$ ~ $\infty$)

$$x' = {{x - x_{mean}} \over {x_{std}}}$$

+ Robust scaling(로버스트 스케일)
	+ IQR = ${x_{q75} - x_{q25}}$
	+ $x_{q50}$ 를 평균 또는 중위수로 사용 가능
$$x' = {{x - x_{q50}} \over {x_{q75} - x_{q25}}}$$


nominal feature : 명목형 데이터로 명칭만 목록 형태로 나타난 데이터
ordinal encoding : 순서형 데이터로 학력, 선호도와 같이 순서에 대한 의미가 존재하는 데이터

**언제 스케일링을 수행해야 하는가?**

1. 주성분 분석(PCA)
	+ 큰 분산 또는 넓은 범위를 가진 변수는 작은 분산을 가지는 변수들보다 주성분 분석에서 큰 계수를 가지게 된다. 즉, 중요하지 않은 변수도 분산이 크면 중요한 변수로 잘못 평가되어 모델링에 방해를 준다.
2. 군집(Clustering)
	+ 군집분석은 거리-기반 알고리즘이기 대문에 거리 측도(ex. 유클리디안, 맨하튼 등)를 이용하여 관측값 사이에 유사성을 찾아 군집을 형성해준다. 만약 넓은 범위를 가진 변수이면 군집에서 큰 영향력을 가지게 된다.
3. K-NN(K-최근접 이웃)
	+ K-NN도 거리 기반 알고리즘이기 대문에 유사성 측도에 모든 변수가 동일하게 상대적인 영향을 계산하기 위해서 스케일링을 수행해야 한다.
4. 서포트벡터머신(SVM)
	+ 서포트벡터머신은 세퍼트벡터와 분류기 사이 거리인 마진(margin)을 최대로 만들어주는 분류기를 찾는 알고리즘이다. 큰 값을 가지는 변수가 거리 계산에 많은 영향을 주므로 스케일링이 필요하다.
5. 회귀
	+ 단순하게 예측을 위한 회귀 모델링이 목적이라면 표준화를 수행할 필요가 없지만, 다변수에서 각 변수의 영향력이 과도하게 커져 다른 변수의 영향력을 다소 약하게 만드는 경우가 있으므로 스케일링이 필요하다.
6. 릿지-라쏘
	+ 릿지-라쏘 모델은 회귀모델에서 제약조건을 가하는 방법으로 다중공선성과 과적합을 해결하기 위해 사용된다. 각 변수의 회귀계수가 제약조건에 영향을 미치기 때문에 스케일링이 필요하다.

**스케일링이 필요하지 않는 경우**
로지스틱 회귀나 트리 기반 모델인 의사결정나무, 랜덤 포레스트, 그래디언트 부스팅, XGBoost, LightGBM는 변수의 크기에 민감하지 않으므로 표준화를 수행할 필요가 없다.

**Feature Importance**
+ Level-wise tree growth를 사용하는 트리 모델들은 적은 레벨에 있는 노드일수록 데이터 분해에 많은 영향을 준다.
+ Leaf-wise tree growth에서는 node split을 수행하는 노드가 얼마나 성능 향상에 영향을 줬는지 알 수 있다.
사용법
1. top K feature selection
2. threshold -> relative importance 값이 특정 값 이상이 되는 피처만 선택

